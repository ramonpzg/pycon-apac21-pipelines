{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "50d25dea-bcce-4c9a-b95f-32a2ae58606d",
   "metadata": {},
   "source": [
    "# 01 ETL Pipelines"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1418debb-7181-419f-9f85-f5aab834e1da",
   "metadata": {},
   "source": [
    "> â€œWithout a systematic way to start and keep data clean, bad data will happen.â€ â€” Donato Diorio"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5154739-1575-4853-9189-e49f044cd76f",
   "metadata": {},
   "source": [
    "![data_flow](https://cdn.dribbble.com/users/1752792/screenshots/5652276/media/12db9ebc672c30dcb4d0fd125f70fb41.png)\n",
    "\n",
    "Source: [Mindaugas SadÅ«nas](https://dribbble.com/shots/5652276-User-flow/attachments/10982649?mode=media)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0031214-e92e-45cf-82da-f9f15d8b9798",
   "metadata": {},
   "source": [
    "## Learning Outcomes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77b7c52a-207b-4de0-b578-c92d6c377b18",
   "metadata": {},
   "source": [
    "By the end of this lesson you will,\n",
    "1. Understand better why we need to move the data from one point to another to the same time that we clean it.\n",
    "2. Understand how to combine data that comes from different sources.\n",
    "3. Have the knowledge of how to create your own data pipelines with Python.\n",
    "4. Learn a little more how to manipulate and mold your data in the way you need it to be.\n",
    "5. Understand how to visualize the pipelines you create to help you with their development."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc4c48f3-0654-440b-b630-1c19519f0ce2",
   "metadata": {},
   "source": [
    "## Tabla de Contenidos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95e2be28-d7fe-49b0-801f-b5a269a91016",
   "metadata": {},
   "source": [
    "1. What are ETL Pipes and Why Should You Learn to Create Them?\n",
    "2. Tools for the Session\n",
    "3. Our case for this workshop\n",
    "4. Data\n",
    "5. Small pipes with ðŸ¼'s 'pipe'\n",
    "6. Extract\n",
    "7. Transform\n",
    "8. Download\n",
    "9. Launch the Pipeline\n",
    "10. Automate it\n",
    "11. Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "629dc5e9-e52e-43a9-8843-d93e7e0b9e9f",
   "metadata": {},
   "source": [
    "## 1. What are ETL Pipelines and why should you learn how to create them?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "427c3972-16d8-4db1-850d-8d0ea08cc875",
   "metadata": {},
   "source": [
    "![etl_pipe](https://databricks.com/wp-content/uploads/2021/05/ETL-Process.jpg)\n",
    "\n",
    "**What are ETL Pipes?**\n",
    "\n",
    "The acronym ETL stands for Extract, Transform, and Load. This is the process the data we consume as analysts, data scientists, scientific researchers, ect ... passes through before it reaches our hands.\n",
    "\n",
    "**Why should you learn to create them?**\n",
    "\n",
    "As data professionals, our task is to create value for our organizations, our clients and our collaborators using all the data that we have at our disposal. However, to get the most out of the data we have on hand, we need\n",
    "1. Information about the process by which the data was generated, For example,\n",
    "    - Point of sale\n",
    "    - Clicks on an online marketplace like Amazon, Etzy, Ebay, ect.\n",
    "    - Epidemiological studies\n",
    "    - ...\n",
    "2. Information about the transformations that occurred during the cleaning and merging process. For instance,\n",
    "    - Celcius degrees were converted to fahrenheit\n",
    "    - Prices in Chilean pesos were converted to {insert your preferred ðŸ’¸}\n",
    "    - Non-numerical and unavailable observations now contain \"Not Available\"\n",
    "    - Numerical observations now contain the average value of its respective variable, for example, a variable with the salary of all employees of a company now contains $40,000$ / year USD in the values that were not available\n",
    "    - ...\n",
    "3. Information about how the data was stored and where. For instance,\n",
    "    - Parquet format\n",
    "    - NOSQL or SQL database\n",
    "    - CSV\n",
    "    - ...\n",
    "\n",
    "Understanding how the three processes described above flow will help us to have more knowledge about the data that we are going to use, and one of the best ways to understand that process is through the creation of data pipelines.\n",
    "\n",
    "**Which Data Professionals Use These Pipes?**\n",
    "\n",
    "- Data scientists\n",
    "- Data analysts\n",
    "- Data Engineers\n",
    "- Machine Learning Engineers\n",
    "- Programmers\n",
    "- DevOps Engineers\n",
    "- Social Sciences Researchers\n",
    "\n",
    "In essence, understanding how data flows in your organization will help you\n",
    "- Give clean data to your analysis while leaving the original data, the source of truth, intact.\n",
    "- Detect inconsistencies in the original data.\n",
    "- Use the time you have to analyze and report on your findings more efficiently."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa4fa45a-d839-4b7a-a9b9-56c4c7823071",
   "metadata": {},
   "source": [
    "## 2. Tools"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ebe98bf-cccc-4817-9c95-fe4a9e874049",
   "metadata": {},
   "source": [
    "The tools that we will use in the workshop are the following.\n",
    "\n",
    "- [pandas](https://pandas.pydata.org/) - \"is a fast, powerful, flexible and easy to use open source data analysis and manipulation tool, built on top of the Python programming language.\"\n",
    "- [Prefect](https://docs.prefect.io/) - \"is a new workflow management system, designed for a modern infrastructure and powered by the open source workflow engine, Prefect Core. Users organize tasks into `Tasks` and` Flows`, and Prefect takes care of the rest.\"\n",
    "- [sqlite3](https://docs.python.org/3/library/sqlite3.html) - \"SQLite is a library written in C that provides a lightweight disk-based database that does not require a separate server process and allows the database to be accessed using a non-standard variant of the SQL query language.\"\n",
    "\n",
    "Before we continue, let's load the modules we'll need and examine a prefect example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8218bb9-af89-42e8-a8b2-da3f72d11b2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from prefect import task, Flow\n",
    "import sqlite3\n",
    "from os.path import join\n",
    "from contextlib import closing\n",
    "from prefect.tasks.database.sqlite import SQLiteScript\n",
    "\n",
    "pd.options.display.max_rows = None\n",
    "pd.options.display.max_columns = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66b1fe20-0851-4625-a154-a57ca7e2f5d4",
   "metadata": {},
   "source": [
    "Imagine we have data on all wildfires between 1983-2020 in the United States.\n",
    "\n",
    "You can find more information about the data [here](https://www.kaggle.com/kkhandekar/total-wildfires-acres-affected-1983-2020)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f604888-8579-4883-a56d-497084a906b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "example_data_in = join(\"..\", \"data\", \"01_part\", \"example\", \"federal_firefighting_costs.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc1565fb-8f3f-438b-ae13-3eccd36dd13b",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.read_csv(example_data_in).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23c8d481-fc31-4b3d-a4f2-9e50ff7ae90c",
   "metadata": {},
   "source": [
    "As you can see, most variables need a bit of fixing since in Python we can't, for example, have numbers with formats like `$70,890`. Also, since we will need the new data every month, we will create an ETL pipeline so that we don't have to repeat the process again and again.\n",
    "\n",
    "When you use prefect you have two important APIs, one is `task` and the other is` Flow`. `task` is used as a decorator on top of functions and allows you to tell prefect that that function will take part in your data pipeline via the` Flow` API.\n",
    "\n",
    "For example, let's create 3 functions, one that extracts the data we need, another that cleans it, and another that downloads it. Each function will the `task`decorator on top of it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edf3ccc3-a96d-4b84-aa6c-6325ac731153",
   "metadata": {},
   "outputs": [],
   "source": [
    "@task\n",
    "def extract(path):\n",
    "    return pd.read_csv(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43947f55-bdce-4308-93eb-dcdf533ae144",
   "metadata": {},
   "source": [
    "As you saw above, only the last 5 variables have commas (`,`) and money symbols (`$`) so we will create a `for` loop and we will replace both with an empty space (` \"\" `).\n",
    "\n",
    "For the download process, we will save the data in the `parquet` format. This is one of the most popular formats as it has a columnar orientation rather than a row orientation.\n",
    "\n",
    "![colvsrow](https://3.bp.blogspot.com/-3aUydn8zCsQ/VjslzWCu3pI/AAAAAAAAAI8/XOi77xQNmm0/s1600/Difference-between-Column-based-and-Row-based-Tables.png)\n",
    "\n",
    "Source: [SAP HANA Central](http://www.hanaexam.com/p/row-store-vs-column-store.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7b387cd-90c7-47a4-9d20-3655f9032514",
   "metadata": {},
   "outputs": [],
   "source": [
    "@task\n",
    "def transform(data):\n",
    "    for col in data.iloc[:, 1:].columns:\n",
    "        data[col] = data[col].str.replace(',', '').str.replace('$', '').astype(int)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5011755-2449-4c78-b9fc-45d4ef8e532b",
   "metadata": {},
   "outputs": [],
   "source": [
    "@task\n",
    "def load(data, path):\n",
    "    data.to_parquet(path, compression='snappy')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8242354e-4d66-4746-b3cb-cf059e0a8eaa",
   "metadata": {},
   "source": [
    "When we have all the steps ready, we create a context manager in Python using the `Flow` API. We can give this function a name, for example, `\" ETL Example \"` and then assign what happens inside our context manager to a variable named `flow` without capital letters (although it can be anything you'd like as well. Inside the context we can instantiate our 3 functions and link them all together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0ba992e-7968-42ba-bfec-ad92e06092e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "example_data_out = join(\"..\", \"data\", \"01_part\", \"example\", \"my_test.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a7b1ec0-0e40-4d5d-94a8-b00118452024",
   "metadata": {},
   "outputs": [],
   "source": [
    "with Flow(\"Ejemplo ETL\") as flow:\n",
    "    data = extract(example_data_in)\n",
    "    data_clean = transform(data)\n",
    "    load(data_clean, example_data_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "350106b9-921a-46c4-8c06-5eade6ab4a37",
   "metadata": {},
   "source": [
    "You can see the result of the steps to follow in our pipeline using `flow.visualize()` and you can start it with `flow.run()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18cb5b3c-8417-4d13-8d15-b2a649282870",
   "metadata": {},
   "outputs": [],
   "source": [
    "flow.visualize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cda05bfa-08ab-4713-b801-8c61c44545ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "flow.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "826f9819-bbab-4e18-9a74-4ee142ce048f",
   "metadata": {},
   "source": [
    "To make sure we have the correct data, let's create a visualization with pandas and hvplot, a package that allows us to add interactivity to our pandas' charts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a32d483-96c3-4998-ad8f-6dd6fd4d3297",
   "metadata": {},
   "outputs": [],
   "source": [
    "import hvplot.pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7bd5051-0822-4448-8e8e-d82fa2221b80",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.read_parquet(join(\"..\", \"data\", \"01_part\", \"example\", \"my_test.parquet\")).hvplot(x='Year', y=\"ForestService\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "734270ca-0595-4989-8e39-707d5a6aee2c",
   "metadata": {},
   "source": [
    "## 3. Case Study"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e33c315e-7854-4149-b7d4-11be818d533a",
   "metadata": {},
   "source": [
    "Imagine you work for a data science consultancy called Beautiful Analytics. Your boss tells you that she has a project for you in which you will work for three governments using data on shared bikes in the cities of London (England, UK), Seoul (South Korea), and Washington (DC, USA). The problem that each government wants to solve is the same,\n",
    "\n",
    "**Challenge #1**\n",
    "\n",
    "> How many bikes do we need to keep available in the city at every hour for the next few years? In other words, how many bikes will be rented at every hour of the day?\n",
    "\n",
    "Each government captures similar data but, as you can imagine, they all use different words and measures in reference to the same variable, which means that our first job before we can answer the question above is to fix the data and put it in amore user-friendly way. By the way, what would really help us a lot is to automate the extraction, transformation and loading of our data since in the future, we will continue to receive the data from the governments. This means that our first real problem is,\n",
    "\n",
    "**Challenge #0**\n",
    "\n",
    "> Create a data pipeline that extracts, transforms and loads the necessary data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28e6fc60-7066-4e65-8d10-1f020aeea86f",
   "metadata": {},
   "source": [
    "## 4. Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "487b3a1f-5b47-47f9-af36-17032159df12",
   "metadata": {},
   "source": [
    "![bikes](https://camo.githubusercontent.com/87d0f6a329d5dd8915136dcf9b121b789bfa613abac31d591f5629cdfb072595/68747470733a2f2f696d672e6b6f72656174696d65732e636f2e6b722f75706c6f61642f6e65777356322f696d616765732f3230323130332f33653962353830316334333034386563613331623333303931373663386461392e6a7067)\n",
    "\n",
    "All three data files contain similar information about how many bicycles have been rented each hour, day, week and month, for several years and for each city.\n",
    "\n",
    "You can get more information about the data of each city using the following links.\n",
    "\n",
    "- [Seoul, Korea del Sur](https://archive.ics.uci.edu/ml/datasets/Seoul+Bike+Sharing+Demand#)\n",
    "- [London, England, UK](https://www.kaggle.com/hmavrodiev/london-bike-sharing-dataset)\n",
    "- [Washington, DC, USA](https://www.kaggle.com/marklvl/bike-sharing-dataset?select=hour.csv)\n",
    "\n",
    "Here are the variables that appear in the three data files.\n",
    "\n",
    "| London | Seoul | Washington |\n",
    "|:------:|:------:|:------:|\n",
    "| date            | date            | instant   |\n",
    "| count           | count           | date      |\n",
    "| temperature     | hour            | seasons   |\n",
    "| temp_feels_like | temperature     | year      |\n",
    "| humidity        | humidity        | month     |\n",
    "| wind_speed      | wind_speed      | hour           |\n",
    "| weather_code    | visibility      | is_holiday     |\n",
    "| is_holiday      | dew_point_temp  | weekday        |\n",
    "| is_weekend      | solar_radiation | workingday     |\n",
    "| seasons         | rainfall        | weathersit     |\n",
    "|                 | snowfall        | temperature    |\n",
    "|                 | seasons         | temp_feels_like |\n",
    "|                 | is_holiday      | humidity        |\n",
    "|                 | functioning_day | wind_speed      |\n",
    "|                 |                 | casual     |\n",
    "|                 |                 | registered |\n",
    "|                 |                 | count      |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c09bdcb-8f1d-4841-a257-47189dca98c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "london_path = join('..', 'data', \"01_part\", 'raw', 'london', 'london_bikes.db')\n",
    "seoul_path = join('..', 'data', \"01_part\", 'raw', 'seoul', 'SeoulBikeData.csv')\n",
    "wash_dc_path = join('..', 'data', \"01_part\", 'raw', 'wash_dc', 'washington.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9de08ae5-aa71-4595-9ef4-ae2dc46f6e5f",
   "metadata": {},
   "source": [
    "We will need to save our new file in a database or in a format in which we are given both what the file occupies and the speed with which we can open and use it. So we will create two paths and two names for the files that we will use later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fc80b44-ad24-4a93-9b30-093bbbab7969",
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_path = join('..', 'data', \"01_part\", 'processed', 'clean.parquet')\n",
    "clean_db_path = join('..', 'data', \"01_part\", 'processed', 'bikes.db')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3395aa14-9426-487a-91e4-cfb233ba8c3b",
   "metadata": {},
   "source": [
    "The data we have about the bikes in London is in a SQLite database and to read it we first need to create a connection to the database. The next step is to use the pandas `read_sql_query` function to read the data. This function takes as an argument two things, the program to grab the data and the connection to the database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef427f4a-d37e-4944-b144-465a5eb440aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "conn = sqlite3.connect(london_path)\n",
    "query = \"SELECT * FROM uk_bikes\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2af05647-6c40-4ae2-8f11-3b086ddcc4b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "london = pd.read_sql_query(query, conn)\n",
    "london.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ed81210-3ce1-418f-9250-ff748b400be4",
   "metadata": {},
   "source": [
    "Seoul data is in text form and separated by commas, and the data for DC is in JSON format. For these two we can use `pd.read_csv` and` pd.read_json`, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67208993-6c95-42eb-afa3-fe35c7aebeda",
   "metadata": {},
   "outputs": [],
   "source": [
    "seoul = pd.read_csv(seoul_path)\n",
    "seoul.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9e583e5-3b38-4f8f-b7ef-835cc93c5dda",
   "metadata": {},
   "outputs": [],
   "source": [
    "washington = pd.read_json(wash_dc_path)\n",
    "washington.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4253750-4e7e-40c5-a2df-193580060fe7",
   "metadata": {},
   "source": [
    "## 5. Data Pipelines with ðŸ¼'s `pipe`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "122909c8-0e1d-4650-8a45-f2ef73495caa",
   "metadata": {},
   "source": [
    "![](https://camo.githubusercontent.com/45ae53e215244585378c3e414ce05abb4f5f6be3/68747470733a2f2f6d656469612e67697068792e636f6d2f6d656469612f4978365150753533576c4236772f67697068792e676966)\n",
    "\n",
    "The `pipe` operator is a pandas function that allows you to chain operations that take a data set, modify it, and return the modified version of the original data. In essence, it allows us to move the data through a series of steps until we reach the structure in which we want to have it.\n",
    "\n",
    "For example, imagine that we have a group of data and 4 functions to fix it, the chain of operations would look like this.\n",
    "\n",
    "```python \n",
    "(data.pipe(change_cols, list_of_cols)\n",
    "     .pipe(clean_numeric_vars, list_of_numeric_vars)\n",
    "     .pipe(add_dates_and_location, 'Auckland', 'NZ')\n",
    "     .pipe(fix_and_drop, 'column_to_fix', seasons_NZ, cols_drop_NZ))\n",
    "```\n",
    "\n",
    "Another way to visualize what happens with pandas' `pipe` is through the following food process where we have ingredients and we actually require food.\n",
    "\n",
    "![img](../images/pandas_pipe.png)\n",
    "\n",
    "Let's start with a small example without `pipe` first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6084c16c-8acc-4cc7-8d1c-b476bf8e2713",
   "metadata": {},
   "outputs": [],
   "source": [
    "toy_data = pd.DataFrame({\"Postal Codes\": [22345, 32442, 20007], \n",
    "                         \"Cities\": [\"Miami\", \"Dallas\", \"Washington\"],\n",
    "                         \"Date\": pd.date_range(start='9/27/2021', periods=3)})\n",
    "toy_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a63c6722-7611-416a-b77b-aa8653f38e8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def change_cols(data, cols_list):\n",
    "    data.columns = cols_list\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "834ed627-12be-4d5d-9fd1-ca30474bc8d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "change_cols(toy_data, [\"postal_code\", \"city\", \"date\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "181234f5-44bf-47b3-a754-907f8609c644",
   "metadata": {},
   "source": [
    "As you can see, with a single function it doesn't make much sense to pipe the process but with a chain of functions, the story changes.\n",
    "\n",
    "And since we have columns with different names, let's create 3 lists with the same names for the three data files since we will need them soon."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "967e5dac-7964-40f9-a711-29fb92f22508",
   "metadata": {},
   "outputs": [],
   "source": [
    "london_cols = ['date', 'count', 'temperature', 'temp_feels_like', 'humidity', 'wind_speed', 'weather_code', 'is_holiday', 'is_weekend', 'seasons']\n",
    "seoul_cols = ['date', 'count', 'hour', 'temperature', 'humidity', 'wind_speed', 'visibility', 'dew_point_temp', 'solar_radiation', 'rainfall', 'snowfall', 'seasons', 'is_holiday', 'functioning_day']\n",
    "wa_dc_cols = ['instant', 'date', 'seasons', 'year', 'month', 'hour', 'is_holiday', 'weekday', 'workingday', 'weathersit', 'temperature', 'temp_feels_like', 'humidity', 'wind_speed', 'casual', 'registered', 'count']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb2fedbc-fb57-435d-8e68-e637a2366475",
   "metadata": {},
   "source": [
    "The next thing we want to do is add additional information about the dates we have for each file. We can achieve this after converting the `date` variable to `datetime` format, which will allow us to access the year, month, week, ect. inside each date."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c71d8b3c-50bc-4659-8a50-2f7dc92eef97",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_dates_and_location(data, city, country):\n",
    "    \n",
    "    data['date'] = pd.to_datetime(data['date'])\n",
    "    data[\"year\"] = data['date'].dt.year\n",
    "    data[\"month\"] = data['date'].dt.month\n",
    "    data[\"week\"] = data['date'].dt.isocalendar().week.astype(int)\n",
    "    data[\"day\"] = data['date'].dt.day\n",
    "    data[\"weekday\"] = data['date'].dt.dayofweek\n",
    "    data[\"is_weekend\"] = (data[\"weekday\"] > 4).astype(int)\n",
    "    # note that we don't want to overwrite data that already has the hour as a column\n",
    "    if 'hour' not in data.columns: \n",
    "        data[\"hour\"] = data['date'].dt.hour\n",
    "    data['date'] = data['date'].dt.date\n",
    "    data['city'] = city\n",
    "    data['country'] = country\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3968e8ef-b70a-44a6-83c0-dae966e96570",
   "metadata": {},
   "outputs": [],
   "source": [
    "add_dates_and_location(toy_data, \"Sydney\", \"AU\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae9545f2-af3d-4933-b031-e6c00a2f2e67",
   "metadata": {},
   "source": [
    "As you can see, we added a lot of information to our data with a simple function, but what happens when we want to chain two or three or four operations together? The following would not be very easy to read right? `add_dates_and_location (change_cols (toy_data, [\" postal_code \",\" city \",\" date \"]),\" Sydney \",\" AU \")`. Let's move on to using our `pipe` operator now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9d83cb9-dfd4-432e-9461-534be6d0cbd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "toy_data = pd.DataFrame({\"Postal Codes\": [22345, 32442, 20007], \n",
    "                         \"Cities\": [\"Miami\", \"Dallas\", \"Washington\"],\n",
    "                         \"Date\": pd.date_range(start='9/27/2021', periods=3)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd338191-61ef-4db6-a537-20e8fa86a3d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    toy_data.pipe(change_cols, [\"zip_code\", \"city\", \"date\"])\n",
    "            .pipe(add_dates_and_location, \"Sydney\", \"AU\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01d4c21b-2919-4d16-94e0-8b3fea8697c4",
   "metadata": {},
   "source": [
    "As you can see, now the chain of our functions is more readable than before and we can continue and chain even more functions in the same fashion.\n",
    "\n",
    "In our data we have the stages of the year with different names and we also have variables that we do not need or that are not in the three files. Let's fix both!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2128c78-d157-4e81-96a4-4cf976290e8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "seasons_london = {0: 'Spring', 1: 'Summer', 2: 'Fall', 3: 'Winter'}\n",
    "seasons_wa_dc = {1: 'Spring', 2: 'Summer', 3: 'Fall', 4: 'Winter'}\n",
    "holidays_seoul = {'No Holiday': 0, 'Holiday': 1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8816693-17ef-4338-910f-53f607ea0f5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_drop_london = ['temp_feels_like', 'weather_code']\n",
    "cols_drop_seoul = ['visibility', 'dew_point_temp', 'solar_radiation', 'rainfall', 'snowfall', 'functioning_day']\n",
    "cols_drop_wa_dc = ['instant', 'temp_feels_like', 'casual', 'registered', 'workingday', 'weathersit']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07cb7869-8908-4c59-a8de-0ba79f7103c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fix_and_drop(data, col_to_fix, mapping, cols_to_drop):\n",
    "    data[col_to_fix] = data[col_to_fix].map(mapping)\n",
    "    return data.drop(cols_to_drop, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54c47580-aed9-4fba-9e14-c5c2210bcdcd",
   "metadata": {},
   "source": [
    "Let's test the `pipe` operator but with the data from DC now and with the list of columns that we created a while ago."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f210a7ef-0e37-4c65-aec1-8d6986c2239b",
   "metadata": {},
   "outputs": [],
   "source": [
    "washington.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e61ae55-01ad-4fda-8d7e-d593bd00141e",
   "metadata": {},
   "outputs": [],
   "source": [
    "(washington.pipe(change_cols, wa_dc_cols)\n",
    "           .pipe(fix_and_drop, 'seasons', seasons_wa_dc, cols_drop_wa_dc)).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4022be4a-27dd-450b-9a04-3e79998b6c11",
   "metadata": {},
   "source": [
    "Lastly, we need to normalize the data for DC as the columns have been altered a bit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddefe01c-40fc-4338-80cd-200488d0dbc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_vars(data):\n",
    "    data['temperature'] = data['temperature'].apply(lambda x: (x * 47) - 8)\n",
    "    data['humidity'] = data['humidity'].apply(lambda x: (x / 100))\n",
    "    data['wind_speed'] = data['wind_speed'].apply(lambda x: (x / 67))\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee3ce8a2-4e9c-4569-ac60-6b0b7e1397cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_data(path):\n",
    "    return pd.read_json(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36ea87cb-97a8-414d-b72f-9e9d3192ee3a",
   "metadata": {},
   "source": [
    "Finally, we can use our `pipe` operator again to complete the process and see the end result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b90f03e1-392b-48cc-b2e3-9174eaac265c",
   "metadata": {},
   "outputs": [],
   "source": [
    "washington = (extract_data(wash_dc_path).pipe(change_cols, wa_dc_cols)\n",
    "                                        .pipe(add_dates_and_location, 'DC', 'USA')\n",
    "                                        .pipe(fix_and_drop, 'seasons', seasons_wa_dc, cols_drop_wa_dc)\n",
    "                                        .pipe(normalize_vars))\n",
    "washington.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c6b2855-b75f-418c-b9a3-9bfc3d8670d3",
   "metadata": {},
   "source": [
    "## Exercise\n",
    "\n",
    "1. Create a function to extract the London data.\n",
    "2. Create a data pipe similar to the one in Washington using pandas' `pipe`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86463bad-efc6-4ac7-bb2a-e5991436ca29",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3d60161-f8f1-4cff-be77-ecd7c0280ebe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28377eea-aa32-4775-b1fd-6adeda091065",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "733222da-f446-4ac5-9f24-5c07b3b345a7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5d4e2dec-e422-4514-b1d2-184dcdd845ef",
   "metadata": {},
   "source": [
    "## 6. Extract"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbf7443d-f824-4be4-bf6d-f9b3f64fae2b",
   "metadata": {},
   "source": [
    "Depending on where the data is, in what format it is stored, and how we can access it, this can either be one of the shortest steps or one of the longest ones in our data pipeline. Here are some of the formats that you might encounter in your day to day.\n",
    "\n",
    "- Text: usually the text format is similar to what we see in Microsoft Excel but without formulas or graphics. For example, CVS or TSV.\n",
    "- JSON: JavaScript Object Notation is a very popular sub-language for its simple syntactics\n",
    "- Databases: These can be SQL, NOSQL, MPP (massively parallel processing), among others.\n",
    "- GeoJSON: It is a type of format for data that contains geographic information. There are many more types of data for GIS.\n",
    "- HTML: Refers to Hyper Text Markup Language and represents the skeleton of almost all web pages in existence.\n",
    "- ...\n",
    "\n",
    "Since we already learned how to create useful pipelines with pandas, we now need to create functions for our main ETL pipeline to automate the process. We can achieve this using the prefect decorator, `@task` from earlier. This decorator takes note of the functions that we want to link together and helps us create a network in which each node is a function and each link connects one or more functions only once.\n",
    "\n",
    "Remember, the decorator `@task` is also a function within prefect and as such, we can pass several arguments to it that help us modify the behavior of each function in our pipeline.\n",
    "\n",
    "You can learn more about the `task` API in the official docs [here](https://docs.prefect.io/core/concepts/tasks.html#overview)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d35f3c5-1e1e-43f2-9ba5-5c320032b4f6",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "task??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f935dd83-0de2-49ab-aaac-ec7bcf1586ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "@task\n",
    "def extract_1(path):\n",
    "    return pd.read_csv(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b63f485-5ec4-49ab-98c7-133bd50e840b",
   "metadata": {},
   "outputs": [],
   "source": [
    "@task\n",
    "def extract_2(path):\n",
    "    conn = sqlite3.connect(path)\n",
    "    query = \"SELECT * FROM uk_bikes\"\n",
    "    return pd.read_sql_query(query, conn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc4b97de-bacd-4700-947f-a283107ae666",
   "metadata": {},
   "outputs": [],
   "source": [
    "@task\n",
    "def extract_3(path):\n",
    "    return pd.read_json(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ca428a5-d2de-4779-814b-442656e8298e",
   "metadata": {},
   "source": [
    "## 7. Transform"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c64fc00-a93c-4270-a9a2-b75e96eb5cfa",
   "metadata": {},
   "source": [
    "The most common transformations that happen at this stage are usually the ones we created earlier. In short,\n",
    "\n",
    "- Clean text data\n",
    "- Normalize columns\n",
    "- Convert numeric variables to the same unit\n",
    "- Deal with missing values\n",
    "- Join data\n",
    "\n",
    "Our transform step will be a parent function to our pipe operators from earlier. Hence, a combination of functions handle by a single one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ad6cc3e-c39a-4d47-aedb-a1893db562d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def order_and_merge(data_lists):\n",
    "    \n",
    "    pick_order = data_lists[0].columns\n",
    "    new_list = [d.reindex(columns=pick_order).sort_values(['date', 'hour']) for d in data_lists]\n",
    "    df = pd.concat(new_list)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "418fd9be-307c-4b0b-9e35-34097139b1ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "@task\n",
    "def transform(london, seoul, washington):\n",
    "    \n",
    "    london = (london.pipe(change_cols, london_cols)\n",
    "                    .pipe(add_dates_and_location, 'London', 'UK')\n",
    "                    .pipe(fix_and_drop, 'seasons', seasons_london, cols_drop_london))\n",
    "    \n",
    "    seoul = (seoul.pipe(change_cols, seoul_cols)\n",
    "                  .pipe(add_dates_and_location, 'Seoul', 'SK')\n",
    "                  .pipe(fix_and_drop, 'is_holiday', holidays_seoul, cols_drop_seoul))\n",
    "    \n",
    "    wash_dc = (washington.pipe(change_cols, wa_dc_cols)\n",
    "                         .pipe(add_dates_and_location, 'DC', 'USA')\n",
    "                         .pipe(fix_and_drop, 'seasons', seasons_wa_dc, cols_drop_wa_dc)\n",
    "                         .pipe(normalize_vars))\n",
    "    \n",
    "    return order_and_merge([london, seoul, wash_dc])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6e1a7f2-056f-451b-9d58-7bf6542f3ea0",
   "metadata": {},
   "source": [
    "## 8. Load"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fc6d2ab-e08a-45bb-aaa5-2407578f8d90",
   "metadata": {},
   "source": [
    "We will have a function for saving the data in a database. For this, Prefect provides us with a wrapper function around SQLite called, `SQLiteScript` which allows us to create a database and run a SQL scipt on top of it. This comes in hady for large and small operations/prototypes alike."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "785c3c96-c261-48d5-83ab-1fe30bf72307",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_table = SQLiteScript(\n",
    "    db=clean_db_path,\n",
    "    script=\"\"\"CREATE TABLE IF NOT EXISTS bike_sharing (date text, count integer, temperature real, humidity real,\n",
    "              wind_speed real, is_holiday real, is_weekend integer, seasons text, year integer,\n",
    "              month integer, week integer, day integer,hour integer, weekday integer, city text,\n",
    "              country text)\"\"\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba3a717e-ad6a-4c03-9889-3d47be517c39",
   "metadata": {},
   "outputs": [],
   "source": [
    "@task\n",
    "def load(data, path_and_name):\n",
    "    \n",
    "    data = list(data.itertuples(name='Bikes', index=False))\n",
    "\n",
    "    insert_cmd = \"INSERT INTO bike_sharing VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)\"\n",
    "    with closing(sqlite3.connect(path_and_name)) as conn:\n",
    "        with closing(conn.cursor()) as cursor:\n",
    "            cursor.executemany(insert_cmd, data)\n",
    "            conn.commit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6ffe40b-8bd1-443c-8f95-5300f282e153",
   "metadata": {},
   "source": [
    "## 9. Lanza La Tuberia"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd3f800a-c8f4-41c1-b275-2c2a5b6d970d",
   "metadata": {},
   "source": [
    "In the same fashion as before, we will combine our functions in a `Flow` with a context manager but this time, we will set up an upstream task with Prefect's `set_upstream()` function as we need our database to be created before we can load the data in it. As you can imagine by the name of the function, it is also possible to set up downstream tasks if need be."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c4e9c43-4c96-47a7-9efe-47f90ce8383e",
   "metadata": {},
   "outputs": [],
   "source": [
    "with Flow('bikes-ETL') as flow:\n",
    "    \n",
    "    the_table = new_table()\n",
    "    \n",
    "    london = extract_2(london_path)\n",
    "    seoul = extract_1(seoul_path)\n",
    "    wash_dc = extract_3(wash_dc_path)\n",
    "    \n",
    "    transformed = transform(london, seoul, wash_dc)\n",
    "        \n",
    "    data_loaded = load(transformed, clean_db_path)\n",
    "    data_loaded.set_upstream(the_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2af9f4a4-c5a8-4c2b-b31c-d6da28fd8198",
   "metadata": {},
   "outputs": [],
   "source": [
    "flow.visualize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e83f8f69-f26b-41e8-a7aa-405be951e210",
   "metadata": {},
   "outputs": [],
   "source": [
    "flow.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e60a30ab-e495-419c-a4f0-6bb6b38844ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.read_sql_query(\"SELECT * FROM bike_sharing\", sqlite3.connect(clean_db_path)).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "714b1086-66ba-42db-b9dd-9929aefe1c82",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Exercise\n",
    "\n",
    "Change the function to unload (`load ()`) and make it save the results in `parquet` format. Run the pipeline again and make sure the results are the same as above by reading the data with pandas' respective function for parquet files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8921afa-8dca-48e2-a8fb-2df5faef110a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43ec06a2-5004-43cc-b657-03ec0f40394d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "265fafb5-db27-492a-b296-93a97c171624",
   "metadata": {},
   "source": [
    "## 10. Automate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a845e3d-3903-45ee-b44b-2eafbd328eda",
   "metadata": {},
   "source": [
    "Our boss tells us that the data of the 3 cities will be updated every Saturday so we have to automate the interval in which we want our program to run. For that we have the `IntervalSchedule` function in prefect, which allows us to set the time interval we need. Whether it's a minute, two weeks, or a month, adding this detail to our pipeline is a trivial task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f807b8c-7895-4f51-abfe-5c736ee05fc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from prefect.schedules import IntervalSchedule\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "865ae5fa-4b7f-4f53-91a2-5d787f182f45",
   "metadata": {},
   "outputs": [],
   "source": [
    "schedule = IntervalSchedule(interval=datetime.timedelta(minutes=1), \n",
    "                            # start_date=datetime.datetime(2021, 11, 5)\n",
    "                           )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e100a922-4b46-4afa-8969-6f6a22c4b062",
   "metadata": {},
   "outputs": [],
   "source": [
    "with Flow('bikes-ETL', schedule=schedule) as flow:\n",
    "    \n",
    "    the_table = new_table()\n",
    "    \n",
    "    london = extract_2(london_path)\n",
    "    seoul = extract_1(seoul_path)\n",
    "    wash_dc = extract_3(wash_dc_path)\n",
    "    \n",
    "    transformed = transform(london, seoul, wash_dc)\n",
    "        \n",
    "    data_loaded = load(transformed, clean_db_path)\n",
    "    data_loaded.set_upstream(the_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9252d0e8-04fc-405f-b747-e9eebc29a0cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "flow.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fb0bb6e-9ca2-4a92-a859-ddf1630ec984",
   "metadata": {},
   "source": [
    "To learn more about how to program and update your pipes, please visit the official documentation at [prefect schedules](https://docs.prefect.io/core/concepts/schedules.html#simple-schedules)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06074f70-0769-4ed1-8b56-6c15e43451ef",
   "metadata": {},
   "source": [
    "## 11. Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3aa474f7-6585-42ee-a4df-3de9743fe279",
   "metadata": {},
   "source": [
    "1. Creating ETL pipes helps you save time cleaning your data.\n",
    "2. pandas `pipe` helps you create chains of functions and save time and lines of code.\n",
    "3. Prefect now time since it chains more functions for you and helps you create schedules for your functions.\n",
    "4. No matter what type of professional you are, moving and cleaning your data is an invaluable tool that is worth knowing."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
