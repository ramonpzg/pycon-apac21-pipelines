{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "182c06ef-fcda-4850-9dc0-4c15af093554",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 04 Full Pipelines with DVC"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d09c1d0-1e72-4d51-a171-976c7703a3c1",
   "metadata": {},
   "source": [
    "> ‚ÄúThe most powerful tool we have as developers is automation.‚Äù ~ Scott Hanselman\n",
    "\n",
    "![dvc_cml](https://i.ytimg.com/vi/H1VBsK7XiKs/maxresdefault.jpg)\n",
    "\n",
    "Image source: [dvc.org](https://dvc.org/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdc92b8c-f5c2-4de9-8e18-2284237fb459",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Synopsis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec361a60-5d9b-43b3-8587-ef5c9038fb66",
   "metadata": {},
   "source": [
    "If you have ever worked with ML pipelines of any kind and wondered if it would be possible to combine the training and model saving parts of it with a CI/CD pipeline, üê≥, look no further as it possible and you will learn about it here. The aim of this tutorial is to show you a step-by-step process for creating reproducible pipelines to gather, clean, train, evaluate, and track machine learning models with DVC (data version control), CML (continuous machine learning), and other tools in the Data Science stack. By the end of this tutorial, you will have the tools to create your own reproducible pipelines and experiment with different tools and models to heart's content."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb175b56-595e-4403-ba83-5a4c636a9b43",
   "metadata": {},
   "source": [
    "## Learning Outcomes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8d19268-51ac-42af-910e-bf6e0edafe48",
   "metadata": {},
   "source": [
    "By the end of the tutorial you would have learned\n",
    "1. a bit of git for keeping track of your code\n",
    "2. a bit of dvc to track your different datasets and machine learning models\n",
    "3. a bit of ML pipelines\n",
    "4. a bit of ML modeling and some python tools for it"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68517f95-bc69-4c75-827c-ce7b91652e62",
   "metadata": {},
   "source": [
    "## Table of Contents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab335fb6-23d9-49df-b679-15571fadae58",
   "metadata": {},
   "source": [
    "1. [Scenario](#Scenario)\n",
    "2. [The Tools](#2.-The-Tools)\n",
    "3. [Environment Set Up](#3.-Environment-Set-Up)\n",
    "4. [The Data](#4.-The-Data)\n",
    "    - [Getting the Data](#4.1-Getting-the-Data)\n",
    "    - [Preparing the Data](#4.2-Preparing-the-Data)\n",
    "5. [Training our First Model](#5.-Training-our-First-Model)\n",
    "6. [Model Evaluation](#6.-Model-Evaluation)\n",
    "7. [DVC Pipelines](#7.-DVC-Pipelines)\n",
    "8. [CI/CD Pipelines with CML](#8.-CI/CD-Pipelines-with-CML)\n",
    "9. [Experiments](#9.-Experiments)\n",
    "10. [Merging our Changes - PRs](#10.-Merging-our-Changes---PRs)\n",
    "11. [Summary](#11.-Summary)\n",
    "12. [Blind Spots and Future Work](#12.-Blind-Spots-and-Future-Work)\n",
    "13. [Resources](#13.-Resources)\n",
    "\n",
    "**NB:** this tutorial was built in a Linux machine and some terminal commands might only be available in \\*NIX-based systems."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fea842b9-e9ef-49f5-93be-1580e8f365c3",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 1. Scenario"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b394fb1e-3bf9-4525-97b6-82898cdbb46e",
   "metadata": {},
   "source": [
    "Imagine you work at a data analytics consultancy called **XYZ Analytics**, and that your boss comes to you with a new challenge for you, to create a machine learning model to predict the amount of bikes neeeded at any given hour of the day in Seoul, South Korea. You don't know anything about bicycle rental systems but you're excited to take on the challenge and accept it with pleasure.\n",
    "\n",
    "![bikes_seoul](https://img.koreatimes.co.kr/upload/newsV2/images/202103/3e9b5801c43048eca31b3309176c8da9.jpg)\n",
    "\n",
    "The challenge was presented to your boss by the South Korean government, and what they are hoping to get later on is an in-house analytical product that anyone can use to figure out the amount of rental bicycles needed at any given time and at different locations in the city of Seoul. You will tackle the predictive modeling part while the rest of team will work on the application and the geospatial part of the task.\n",
    "\n",
    "Lastly, XYZ Analytics has been improving their data science capabilities and would like for every project to use data and model version control tools, which means you will be using dvc and other cool tools for the first time for this task. Let's go over the tooling in the next section."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc372370-9f64-4fac-a64e-184513a735bf",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 2. The Tools"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28af17fe-063c-44a3-bb9a-7dcf7ac4290a",
   "metadata": {},
   "source": [
    "Here are some of the tools that we will be using.\n",
    "\n",
    "- [DVC](https://dvc.org/) - \"Data Version Control, or DVC, is a data and ML experiment management tool that takes advantage of the existing engineering toolset that you're already familiar with (Git, CI/CD, etc.).\"\n",
    "- [NumPy](https://numpy.org/) - \"It is a Python library that provides a multidimensional array object, various derived objects (such as masked arrays and matrices), and an assortment of routines for fast operations on arrays, including mathematical, logical, shape manipulation, sorting, selecting, I/O, discrete Fourier transforms, basic linear algebra, basic statistical operations, random simulation and much more.\"\n",
    "- [pandas](https://pandas.pydata.org/) - \"is a fast, powerful, flexible and easy to use open source data analysis and manipulation tool, built on top of the Python programming language.\"\n",
    "- [scikit-learn](https://scikit-learn.org/stable/index.html) - \"is an open source machine learning library that supports supervised and unsupervised learning. It also provides various tools for model fitting, data preprocessing, model selection and evaluation, and many other utilities.\"\n",
    "- [XGBoost](https://xgboost.readthedocs.io/en/latest/index.html) - \"is an optimized distributed gradient boosting library designed to be highly efficient, flexible and portable. It implements machine learning algorithms under the Gradient Boosting framework. XGBoost provides a parallel tree boosting (also known as GBDT, GBM) that solve many data science problems in a fast and accurate way.\"\n",
    "- [LightGBM](https://lightgbm.readthedocs.io/en/latest/index.html) - \"is a gradient boosting framework that uses tree based learning algorithms. It is designed to be distributed and efficient with the following advantages: Faster training speed and higher efficiency, lower memory usage, better accuracy, support of parallel, distributed, and GPU learning, and capable of handling large-scale data.\"\n",
    "- [CatBoost](https://catboost.ai/en/docs/) - \"CatBoost is a machine learning algorithm that uses gradient boosting on decision trees. It is available as an open source library.\"\n",
    "- [Git](https://git-scm.com/) - \"Git is a free and open source distributed version control system designed to handle everything from small to very large projects with speed and efficiency.\"\n",
    "\n",
    "Let's now set up our development environment and get started with our project.\n",
    "\n",
    "**NB:** the definitions above have been taken directly from the tools' respective websites."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a2aeb89-667c-4da3-ab20-a1e99c2f9357",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 3. Environment Set Up"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dacc120-e6c8-416e-8ea3-b7d3b51224c0",
   "metadata": {},
   "source": [
    "Usually we want to work with a directory with a somewhat similar structure to the one below but for the purpose of this tutorial, the set up will be slightly different.\n",
    "\n",
    "```bash\n",
    ".\n",
    "‚îú‚îÄ‚îÄ data\n",
    "‚îÇ¬†¬† ‚îú‚îÄ‚îÄ processed\n",
    "‚îÇ¬†¬† ‚îÇ¬†¬† ‚îú‚îÄ‚îÄ test\n",
    "‚îÇ¬†¬† ‚îÇ¬†¬† ‚îî‚îÄ‚îÄ train\n",
    "‚îÇ¬†¬† ‚îî‚îÄ‚îÄ raw\n",
    "‚îú‚îÄ‚îÄ metrics\n",
    "‚îú‚îÄ‚îÄ models\n",
    "‚îú‚îÄ‚îÄ notebooks\n",
    "‚îú‚îÄ‚îÄ src\n",
    "‚îî‚îÄ‚îÄ README.md\n",
    "```\n",
    "\n",
    "\n",
    "Let's now initialize our git and dvc repository with the following commands.\n",
    "\n",
    "```bash\n",
    "git init\n",
    "\n",
    "dvc init\n",
    "```\n",
    "\n",
    "You should see the following output.\n",
    "\n",
    "![gitdvc](../images/git_dvc.png)\n",
    "\n",
    "Now that we have everything we need, we can open up our IDE using the `jupyter lab` command in the terminal. (You should see the following output minus the README.md file.)\n",
    "\n",
    "![jlab](../images/jupyterlab.png)\n",
    "\n",
    "After you open Jupyter Lab, create a notebook in the notebooks directory and call it `exploration.ipynb`. The rest of the tutorial can, and will be, done through the jupyter notebook we just created. Let's dive in.\n",
    "\n",
    "**NB:** You can get `conda` through the miniconda distribution [here](https://docs.conda.io/en/latest/miniconda.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb5243e0-f99c-4a20-b073-5a47ff20108f",
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b529cc80-17b8-480c-b9d9-e06b19cadd87",
   "metadata": {},
   "outputs": [],
   "source": [
    "!git init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "733328b9-7270-45ec-bf17-772c268a6429",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized DVC repository.\n",
      "\n",
      "You can now commit the changes to git.\n",
      "\n",
      "\u001b[31m+---------------------------------------------------------------------+\n",
      "\u001b[0m\u001b[31m|\u001b[0m                                                                     \u001b[31m|\u001b[0m\n",
      "\u001b[31m|\u001b[0m        DVC has enabled anonymous aggregate usage analytics.         \u001b[31m|\u001b[0m\n",
      "\u001b[31m|\u001b[0m     Read the analytics documentation (and how to opt-out) here:     \u001b[31m|\u001b[0m\n",
      "\u001b[31m|\u001b[0m             <\u001b[36mhttps://dvc.org/doc/user-guide/analytics\u001b[39m>              \u001b[31m|\u001b[0m\n",
      "\u001b[31m|\u001b[0m                                                                     \u001b[31m|\u001b[0m\n",
      "\u001b[31m+---------------------------------------------------------------------+\n",
      "\u001b[0m\n",
      "\u001b[33mWhat's next?\u001b[39m\n",
      "\u001b[33m------------\u001b[39m\n",
      "- Check out the documentation: <\u001b[36mhttps://dvc.org/doc\u001b[39m>\n",
      "- Get help and share ideas: <\u001b[36mhttps://dvc.org/chat\u001b[39m>\n",
      "- Star us on GitHub: <\u001b[36mhttps://github.com/iterative/dvc\u001b[39m>\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!dvc init"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a09ed3dc-4b3d-442a-aa25-f68619e7b1a2",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 4. The Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4208ae1-2637-4cb2-a7d8-b75b6d5f929d",
   "metadata": {},
   "source": [
    "Following the described scenario above, the data was donated to the UCI ML Repository on 2020-03-01 for a regression task. It contains information regarding the amount of bikes available per hour between 2017 and 2018.\n",
    "\n",
    "![bikes_uci](../images/uci_bikes.png)\n",
    "\n",
    "Here are the variables found in the dataset.\n",
    "\n",
    "- `Date` - year-month-day\n",
    "- `Rented Bike count` - Count of bikes rented at each hour\n",
    "- `Hour` - Hour of the day\n",
    "- `Temperature`-Temperature in Celsius\n",
    "- `Humidity` - %\n",
    "- `Windspeed` - m/s\n",
    "- `Visibility` - 10m\n",
    "- `Dew point temperature` - Celsius\n",
    "- `Solar radiation` - MJ/m2\n",
    "- `Rainfall` - mm\n",
    "- `Snowfall` - cm\n",
    "- `Seasons` - Winter, Spring, Summer, and Autumn\n",
    "- `Holiday` - Holiday and No holiday\n",
    "- `Functional Day` - NoFunc (Non Functional Hours), Fun (Functional hours)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01f8fd2c-741e-4656-8f8a-62b3585f96e6",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 4.1 Getting the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17facbd6-b65e-41ff-9b42-e8ebc39a9704",
   "metadata": {},
   "source": [
    "We will use the `urllib.request` and the `os` libraries to download the data and set up our desired path for it, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0061face-8fbd-4a62-b477-fa7472520bf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request, os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "91c0632b-3224-4f5d-a328-4471799a0ca8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ramonperez/Tresors/datascience/tutorials/pycon_apac21/notebooks\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5d136262-263d-47d6-8bac-3c070001eb22",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(\"..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9cbc15da-8598-431a-ad3e-87217fe64584",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ramonperez/Tresors/datascience/tutorials/pycon_apac21\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a4e0fac-7166-4b69-8614-2958cd95c79b",
   "metadata": {},
   "source": [
    "The dataset can be downloaded from the URL below and we will give it the filename, `SeoulBikeData.csv`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1e017317-2ef4-4756-95aa-9f5c1f8bbe50",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://archive.ics.uci.edu/ml/machine-learning-databases/00560/SeoulBikeData.csv'\n",
    "path_and_filename = os.path.join('data', '03_part', 'raw', 'SeoulBikeData.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8122ea37-6f1e-440d-b868-559e486a2875",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('data/03_part/raw/SeoulBikeData.csv',\n",
       " <http.client.HTTPMessage at 0x7f94306b5280>)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "urllib.request.urlretrieve(url, path_and_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fbb66c6-b3dc-4905-a414-86a7a99bc8c6",
   "metadata": {},
   "source": [
    "Because we want to be able to create a pipeline later on, we will export the few lines of code above as a python script called `get_data.py` for later use. We will put every python script we want in our pipeline inside the `src` directory. Get used to this pattern. :)\n",
    "\n",
    "Also, it is good practice to make sure the directories we are using always exist, so we will add an additional if-else statement to search and/or create it if it does not exist.\n",
    "\n",
    "The command `%%writefile` below is a magic function and these are special function of ipython interpreter. The one we are using allows us to write anything in that cell to a file. Others like the `%%bash`, as we will soon see, make the entire cell a bash executable cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "89532f82-87b4-4bcb-8381-d228f7425555",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting src/full_pipe/get_data.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile src/full_pipe/get_data.py\n",
    "\n",
    "import urllib.request, os\n",
    "\n",
    "url = 'https://archive.ics.uci.edu/ml/machine-learning-databases/00560/SeoulBikeData.csv'\n",
    "path = os.path.join('data', '03_part', 'raw')\n",
    "filename = 'SeoulBikeData.csv'\n",
    "\n",
    "if not os.path.exists(path): os.makedirs(path)\n",
    "        \n",
    "urllib.request.urlretrieve(url, os.path.join(path, filename))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36a7112e-6850-4b89-9495-479f1d238d6b",
   "metadata": {},
   "source": [
    "Now that we have our dataset, let's go ahead and add it to our remote storage and start keeping track of the changes that we make to it with dvc. We will use s3 as our primary storage tool for the tutorial but feel free to use the option that best suits your needs and experience from the [dvc website](https://dvc.org/doc/command-reference/remote/add). Here are the step to do it with AWS.\n",
    "\n",
    "1. Log into your AWS account\n",
    "2. Navigate to **Identity and Access Management (IAM)** > **Access Management** > **Users**\n",
    "3. Click on **Add users**\n",
    "4. Add a name under **Set user details**, e.g. `bikes`\n",
    "5. In the **Select AWS access type**, check the box [x] next to **Access key - Programmatic access**\n",
    "6. In the **Set permissions** section, click on **Attach existing policies directly** and search and select **AmazonS3FullAccess**\n",
    "7. Accept the rest of the defaults, create your user and download the csv file with your **Access key ID** and your **Secret access key**\n",
    "8. Navigate to the **S3 Management Console** and create a new bucket with the default settings, mine is **bikesdata**\n",
    "9. Navigate to your bucket's **Permissions** tab and in the **Bucket policy** section click on **Edit** and the click on **Policy generator**\n",
    "    - In the **Select Type of Policy** select **S3 Bucket Policy**\n",
    "    - In the **Principal** box add your user's arn code, e.g. `arn:aws:iam::123456789135:user/bikes`\n",
    "    - In the **Actions** box, select ListBucket\n",
    "    - In the **Amazon Resource Name** add your bucket's arn, e.g. `arn:aws:s3:::bikesdata`\n",
    "    - Click on **Add Statement**\n",
    "    - Click on **Generate Policy** and then copy **Policy JSON Document**\n",
    "![bucketpolicy](../images/policy_json.png)\n",
    "    - Add the policy to your bucket and save the changes\n",
    "   \n",
    "Our Bucket without the data should look as follows.\n",
    "![empty_bucket](../images/empty_bucket.png)\n",
    "    \n",
    "Now let's add our bucket to our dvc repo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5c766232-1858-4909-9c9f-fb050fefce60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting 'my_drive' as a default remote.\n"
     ]
    }
   ],
   "source": [
    "!dvc remote add -d bikestorage gdrive://1hhbxsGSxrVRcJaxI8_cZ9wroJzo3DsVy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c9aa0be8-c133-49bd-9880-f5b88eef02e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[32m‚†ô\u001b[0m Checking graph                                                   \u001b[32m‚†ã\u001b[0m Checking graph\n",
      "Adding...                                                                       \n",
      "!\u001b[A\n",
      "  0%|          |                                   0.00/? [00:00<?,        ?B/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "!\u001b[A\n",
      "  0%|          |.RLigE5i2k6uE6Sd9kjJKTZ.tmp           0/1 [00:00<?,       ?it/s]\u001b[A\n",
      "100% Adding...|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà|1/1 [00:00, 32.05file/s]\u001b[A\n",
      "\n",
      "To track the changes with git, run:\n",
      "\n",
      "\tgit add data/03_part/raw/SeoulBikeData.csv.dvc data/03_part/raw/.gitignore\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!dvc add data/03_part/raw/SeoulBikeData.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26df8927-6bd7-48e6-9ebd-2f225c30c85e",
   "metadata": {},
   "source": [
    "You will have to run `dvc push` on the terminal, and after that, \n",
    "\n",
    "In the first line above the `-d` stands for default and `bikestorage` is the name we have decided on for our bucket. The last piece is the url that directs dvc to our s3 bucket. You can find out more about the `remote` command through the official docummentation [here](https://dvc.org/doc/command-reference/remote).\n",
    "\n",
    "In the next few lines, where we use the `modify` command to update our dvc local files, we give dvc the resources necessary to access our remote storage from our local computer. If you were to have the `awscli` installed and configured in your machine, you could have skipped the modify parts of the above cell. You can find more about the `modify` command through the official docummentation [here](https://dvc.org/doc/command-reference/remote/modify)\n",
    "\n",
    "Make sure to always keep your credentials in a safe and secure place.\n",
    "\n",
    "Now let's start tracking our data and make sure our remote storage is fully connected to our local storage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6e465248-b384-4a7d-9a4b-8a5e19350abc",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "git add data/03_part/raw/SeoulBikeData.csv.dvc data/03_part/raw/.gitignore"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71479fa0-ab62-4f28-a12a-651bb144812e",
   "metadata": {},
   "source": [
    "![fullbucket](../images/file_up.png)\n",
    "\n",
    "DVC uses special names to keep track of files, so there's no need to try and figure out what the above name means. Everything in our bucket can always be accessed through dvc.\n",
    "\n",
    "Lastly, we'll commit our changes to our git repo after making sure we add the two files created by dvc, `data/raw/.gitignore` and `data/raw/SeoulBikeData.csv.dvc`. What dvc is doing is tracking some information about our dataset through git, hence the files `...Data.csv.dvc` and `.gitignore` with the actual data file, while the actual data goes to our remote storage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "40eeca7e-84f8-410d-adcc-8682bddb27d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "On branch master\n",
      "Your branch is up to date with 'origin/master'.\n",
      "\n",
      "Changes to be committed:\n",
      "  (use \"git restore --staged <file>...\" to unstage)\n",
      "\t\u001b[32mnew file:   data/03_part/raw/.gitignore\u001b[m\n",
      "\t\u001b[32mnew file:   data/03_part/raw/SeoulBikeData.csv.dvc\u001b[m\n",
      "\n",
      "Changes not staged for commit:\n",
      "  (use \"git add <file>...\" to update what will be committed)\n",
      "  (use \"git restore <file>...\" to discard changes in working directory)\n",
      "\t\u001b[31mmodified:   .dvc/config\u001b[m\n",
      "\t\u001b[31mmodified:   notebooks/03_dvc_pipes.ipynb\u001b[m\n",
      "\n",
      "Untracked files:\n",
      "  (use \"git add <file>...\" to include in what will be committed)\n",
      "\t\u001b[31mimages/dvc_push.png\u001b[m\n",
      "\t\u001b[31mimages/file_up.png\u001b[m\n",
      "\t\u001b[31mimages/gdrive1.png\u001b[m\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!git status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "09d85c8d-97c7-45e0-af89-5211823d01f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[master 8359d71] Start Tracking Data\n",
      " 2 files changed, 5 insertions(+)\n",
      " create mode 100644 data/03_part/raw/.gitignore\n",
      " create mode 100644 data/03_part/raw/SeoulBikeData.csv.dvc\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "git commit -m \"Start Tracking Data\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcb24e02-eb5b-40ed-9455-e52869833a5a",
   "metadata": {},
   "source": [
    "Before we push any changes to GitHub, make sure you create your repository, as shown below, and then connect your local and remote repo with the commands below.\n",
    "![reposetup](../images/repo_setup.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b874731c-a946-4ee4-a77f-10454647a172",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "To github.com:ramonpzg/pycon-apac21-pipelines.git\n",
      "   035163e..8359d71  master -> master\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "# git remote add origin https://github.com/ramonpzg/bikes_ml.git\n",
    "git push -u origin master"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73e94145-6d60-43d2-a648-47e146154b70",
   "metadata": {},
   "source": [
    "After pushing the changes to GitHub you should see the files "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c260fcd-a8ff-4fae-968c-d27e6d6bac7a",
   "metadata": {},
   "source": [
    "### 4.2 Preparing the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97c08628-20c4-4459-8e96-45a04de65256",
   "metadata": {},
   "source": [
    "The following steps should feel familiar to us, we want to separate the date variable into its components, create dummy variables for the categorical features, normalize the columns so that they only contain alphanumerical characters with underscores instead of spaces, and finally split the data into train and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d801a38d-bb07-4811-8d7d-464a2f0b7e02",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Rented Bike Count</th>\n",
       "      <th>Hour</th>\n",
       "      <th>Temperature(¬∞C)</th>\n",
       "      <th>Humidity(%)</th>\n",
       "      <th>Wind speed (m/s)</th>\n",
       "      <th>Visibility (10m)</th>\n",
       "      <th>Dew point temperature(¬∞C)</th>\n",
       "      <th>Solar Radiation (MJ/m2)</th>\n",
       "      <th>Rainfall(mm)</th>\n",
       "      <th>Snowfall (cm)</th>\n",
       "      <th>Seasons</th>\n",
       "      <th>Holiday</th>\n",
       "      <th>Functioning Day</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>01/12/2017</td>\n",
       "      <td>254</td>\n",
       "      <td>0</td>\n",
       "      <td>-5.2</td>\n",
       "      <td>37</td>\n",
       "      <td>2.2</td>\n",
       "      <td>2000</td>\n",
       "      <td>-17.6</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Winter</td>\n",
       "      <td>No Holiday</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>01/12/2017</td>\n",
       "      <td>204</td>\n",
       "      <td>1</td>\n",
       "      <td>-5.5</td>\n",
       "      <td>38</td>\n",
       "      <td>0.8</td>\n",
       "      <td>2000</td>\n",
       "      <td>-17.6</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Winter</td>\n",
       "      <td>No Holiday</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>01/12/2017</td>\n",
       "      <td>173</td>\n",
       "      <td>2</td>\n",
       "      <td>-6.0</td>\n",
       "      <td>39</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2000</td>\n",
       "      <td>-17.7</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Winter</td>\n",
       "      <td>No Holiday</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>01/12/2017</td>\n",
       "      <td>107</td>\n",
       "      <td>3</td>\n",
       "      <td>-6.2</td>\n",
       "      <td>40</td>\n",
       "      <td>0.9</td>\n",
       "      <td>2000</td>\n",
       "      <td>-17.6</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Winter</td>\n",
       "      <td>No Holiday</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>01/12/2017</td>\n",
       "      <td>78</td>\n",
       "      <td>4</td>\n",
       "      <td>-6.0</td>\n",
       "      <td>36</td>\n",
       "      <td>2.3</td>\n",
       "      <td>2000</td>\n",
       "      <td>-18.6</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Winter</td>\n",
       "      <td>No Holiday</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Date  Rented Bike Count  Hour  Temperature(¬∞C)  Humidity(%)  \\\n",
       "0  01/12/2017                254     0             -5.2           37   \n",
       "1  01/12/2017                204     1             -5.5           38   \n",
       "2  01/12/2017                173     2             -6.0           39   \n",
       "3  01/12/2017                107     3             -6.2           40   \n",
       "4  01/12/2017                 78     4             -6.0           36   \n",
       "\n",
       "   Wind speed (m/s)  Visibility (10m)  Dew point temperature(¬∞C)  \\\n",
       "0               2.2              2000                      -17.6   \n",
       "1               0.8              2000                      -17.6   \n",
       "2               1.0              2000                      -17.7   \n",
       "3               0.9              2000                      -17.6   \n",
       "4               2.3              2000                      -18.6   \n",
       "\n",
       "   Solar Radiation (MJ/m2)  Rainfall(mm)  Snowfall (cm) Seasons     Holiday  \\\n",
       "0                      0.0           0.0            0.0  Winter  No Holiday   \n",
       "1                      0.0           0.0            0.0  Winter  No Holiday   \n",
       "2                      0.0           0.0            0.0  Winter  No Holiday   \n",
       "3                      0.0           0.0            0.0  Winter  No Holiday   \n",
       "4                      0.0           0.0            0.0  Winter  No Holiday   \n",
       "\n",
       "  Functioning Day  \n",
       "0             Yes  \n",
       "1             Yes  \n",
       "2             Yes  \n",
       "3             Yes  \n",
       "4             Yes  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = pd.read_csv(os.path.join('data', '03_part', 'raw', 'SeoulBikeData.csv'), encoding='iso-8859-1')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ab585257-f164-4dc4-89a5-5cf94a6fb1d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['Date'] = pd.to_datetime(data['Date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "47fbada1-711d-4bad-8fb8-3cfb1096afe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.sort_values(['Date', 'Hour'], inplace=True)\n",
    "data[\"Year\"] = data['Date'].dt.year\n",
    "data[\"Month\"] = data['Date'].dt.month\n",
    "data[\"Week\"] = data['Date'].dt.isocalendar().week\n",
    "data[\"Day\"] = data['Date'].dt.day\n",
    "data[\"Dayofweek\"] = data['Date'].dt.dayofweek\n",
    "data[\"Dayofyear\"] = data['Date'].dt.dayofyear\n",
    "data[\"Is_month_end\"] = data['Date'].dt.is_month_end\n",
    "data[\"Is_month_start\"] = data['Date'].dt.is_month_start\n",
    "data[\"Is_quarter_end\"] = data['Date'].dt.is_quarter_end\n",
    "data[\"Is_quarter_start\"] = data['Date'].dt.is_quarter_start\n",
    "data[\"Is_year_end\"] = data['Date'].dt.is_year_end\n",
    "data[\"Is_year_start\"] = data['Date'].dt.is_year_start\n",
    "data.drop('Date', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f3491f53-17c0-4edc-9e22-8e352f08fe78",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.get_dummies(data=data, columns=['Holiday', 'Seasons', 'Functioning Day'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bb490f64-7264-4b6b-82a8-f9847b1cb872",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.columns = ['rented_bike_count', 'hour', 'temperature', 'humidity', 'wind_speed', 'visibility', \n",
    "                'dew_point_temperature', 'solar_radiation', 'rainfall', 'snowfall', 'year', \n",
    "                'month', 'week', 'day', 'dayofweek', 'dayofyear', 'is_month_end', 'is_month_start',\n",
    "                'is_quarter_end', 'is_quarter_start', 'is_year_end', 'is_year_start',\n",
    "                'seasons_autumn', 'seasons_winter', 'seasons_summer', 'seasons_spring',\n",
    "                'holiday_yes', 'holiday_no', 'functioning_day_no', 'functioning_day_yes']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5fc5eff2-566e-4155-8e78-630dc204c0d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "split = 0.30\n",
    "n_train = int(len(data) - len(data) * split)\n",
    "\n",
    "train_path = os.path.join('data', '03_part', 'processed', 'train.csv')\n",
    "test_path = os.path.join('data', '03_part', 'processed', 'test.csv')\n",
    "\n",
    "data[:n_train].reset_index(drop=True).to_csv(train_path, index=False)\n",
    "data[n_train:].reset_index(drop=True).to_csv(test_path, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5da31f9-2d67-4a29-96b1-ded03e2481b0",
   "metadata": {},
   "source": [
    "Using the same commands as before, let's keep track of our new dataset with dvc and push the changes to our s3 bucket. In addition, we'll create a file called `prepared.py` for later use in our pipelines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7ce3a228-463e-471d-9ab4-e7159c452412",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "To track the changes with git, run:\n",
      "\n",
      "\tgit add data/03_part/processed/test.csv.dvc data/03_part/processed/.gitignore data/03_part/processed/train.csv.dvc\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "dvc add data/03_part/processed/train.csv data/03_part/processed/test.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6ff7b8c7-7d43-4394-b7b4-22d97ce10b82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting src/full_pipe/prepare.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile src/full_pipe/prepare.py\n",
    "\n",
    "import pandas as pd\n",
    "import os, sys\n",
    "\n",
    "split = 0.30\n",
    "\n",
    "raw_data_path = sys.argv[1]\n",
    "train_path = os.path.join('data', '03_part', 'processed', 'train.csv')\n",
    "test_path = os.path.join('data', '03_part', 'processed', 'test.csv')\n",
    "\n",
    "# read the data\n",
    "data = pd.read_csv(raw_data_path, encoding='iso-8859-1')\n",
    "\n",
    "# add date vars\n",
    "data['Date'] = pd.to_datetime(data['Date'])\n",
    "data.sort_values(['Date', 'Hour'], inplace=True)\n",
    "data[\"Year\"] = data['Date'].dt.year\n",
    "data[\"Month\"] = data['Date'].dt.month\n",
    "data[\"Week\"] = data['Date'].dt.isocalendar().week\n",
    "data[\"Day\"] = data['Date'].dt.day\n",
    "data[\"Dayofweek\"] = data['Date'].dt.dayofweek\n",
    "data[\"Dayofyear\"] = data['Date'].dt.dayofyear\n",
    "data[\"Is_month_end\"] = data['Date'].dt.is_month_end\n",
    "data[\"Is_month_start\"] = data['Date'].dt.is_month_start\n",
    "data[\"Is_quarter_end\"] = data['Date'].dt.is_quarter_end\n",
    "data[\"Is_quarter_start\"] = data['Date'].dt.is_quarter_start\n",
    "data[\"Is_year_end\"] = data['Date'].dt.is_year_end\n",
    "data[\"Is_year_start\"] = data['Date'].dt.is_year_start\n",
    "data.drop('Date', axis=1, inplace=True)\n",
    "\n",
    "# add dummies\n",
    "data = pd.get_dummies(data=data, columns=['Holiday', 'Seasons', 'Functioning Day'])\n",
    "\n",
    "# Normalize columns\n",
    "data.columns = ['rented_bike_count', 'hour', 'temperature', 'humidity', 'wind_speed', 'visibility', \n",
    "                'dew_point_temperature', 'solar_radiation', 'rainfall', 'snowfall', 'year', \n",
    "                'month', 'week', 'day', 'dayofweek', 'dayofyear', 'is_month_end', 'is_month_start',\n",
    "                'is_quarter_end', 'is_quarter_start', 'is_year_end', 'is_year_start',\n",
    "                'seasons_autumn', 'seasons_winter', 'seasons_summer', 'seasons_spring',\n",
    "                'holiday_yes', 'holiday_no', 'functioning_day_no', 'functioning_day_yes']\n",
    "\n",
    "n_train = int(len(data) - len(data) * split)\n",
    "data[:n_train].reset_index(drop=True).to_csv(train_path, index=False)\n",
    "data[n_train:].reset_index(drop=True).to_csv(test_path, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f63708ba-3d1f-440a-a9d3-6d31b1178555",
   "metadata": {},
   "source": [
    "Now we are ready to commit all of our changes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f468cb08-e2b3-456b-8a99-7491d1fe6bba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[master c1ffbd5] Preparation stage completed\n",
      " 8 files changed, 120 insertions(+), 38 deletions(-)\n",
      " create mode 100644 data/03_part/processed/.gitignore\n",
      " create mode 100644 data/03_part/processed/test.csv.dvc\n",
      " create mode 100644 data/03_part/processed/train.csv.dvc\n",
      " create mode 100644 images/dvc_push.png\n",
      " create mode 100644 images/file_up.png\n",
      " create mode 100644 images/gdrive1.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "To github.com:ramonpzg/pycon-apac21-pipelines.git\n",
      "   8359d71..c1ffbd5  master -> master\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "git add .\n",
    "git commit -m \"Preparation stage completed\"\n",
    "git push"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69fb0768-f72c-4a52-bf5f-366dd7cb9d77",
   "metadata": {},
   "source": [
    "## 5. Training our First Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad274b95-b346-4d8f-8a0f-6626ae45c62a",
   "metadata": {},
   "source": [
    "We want to create a model that predicts how many bikes will be needed at any given hour and on any given date in the future in the city of Seoul. Since the number of bicycles available for rent is a continuous number, this is a regression problem and what a better tool to use for regression problems that Random Forests.\n",
    "\n",
    "![rfs](https://media.makeameme.org/created/its-easy-just-5bd65c.jpg)\n",
    "\n",
    "What are Random Forests anyways?\n",
    "\n",
    "> \"Random forests or random decision forests are an ensemble learning method for classification, regression and other tasks that operates by constructing a multitude of decision trees at training time. For classification tasks, the output of the random forest is the class selected by most trees. For regression tasks, the mean or average prediction of the individual trees is returned. Random decision forests correct for decision trees' habit of overfitting to their training set.\" ~ [Wikipedia](https://en.wikipedia.org/wiki/Random_forest)\n",
    "\n",
    "We want to start with a baseline model, evaluate it, and then fine tune either the implementation that we picked, in this case the scikit-learn's one or, as we'll see in a later section, an implementation from another framework.\n",
    "\n",
    "After we train our sklearn model, we want to serialize (or pickle) that model, track it with dvc, and use it later with unseen data in the evaluation stage.\n",
    "\n",
    "We'll import sklearn's `RandomForestRegressor` and python's `pickle` module, load our train set, and start our training with 100 estimators and seed. Feel free to change these however you'd like tho."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3953d600-16bf-4a07-b19c-eafd1cf3559b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2d46855d-c5a0-4f2b-b35f-992581bb4128",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = pd.read_csv(os.path.join('data', '03_part', 'processed', 'train.csv'))\n",
    "y_train = X_train.pop('rented_bike_count')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "02fe66a5-14d1-41d7-bef2-425c9dd59e19",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 42\n",
    "n_est = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ac3cb0d2-71dc-4377-bcf9-bf7eb820195a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestRegressor(random_state=42)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf = RandomForestRegressor(n_estimators=n_est, random_state=seed)\n",
    "rf.fit(X_train.values, y_train.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7e2ce51c-ba62-41af-ab51-2c581ab06d5c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([230.73, 214.51, 163.6 ,  96.76,  79.29,  90.66, 158.14, 480.86,\n",
       "       791.04, 466.79])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf.predict(X_train.values)[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "2f1527f8-cc31-44c6-ad83-cc62db880d17",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('models/rf_model.pkl', \"wb\") as fd:\n",
    "    pickle.dump(rf, fd)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0842c9a3-4409-42c1-8fcb-b93d19b22f78",
   "metadata": {},
   "source": [
    "Now that we have a trained model, let's save the steps we just took to a file called `train.py`, and let's also start tracking our model in the same way in which we tracked our data earlier with dvc. Lastly, we'll commit our work and push everything to GitHub."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "60b4559f-887d-46d1-bff0-1fa417947eb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting src/full_pipe/train.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile src/full_pipe/train.py\n",
    "\n",
    "import os, pickle, sys\n",
    "import numpy as np, pandas as pd\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "input_data = sys.argv[1]\n",
    "output = os.path.join('models', 'rf_model.pkl')\n",
    "seed = 42\n",
    "n_est = 100\n",
    "\n",
    "X_train = pd.read_csv(input_data)\n",
    "y_train = X_train.pop('rented_bike_count')\n",
    "\n",
    "rf = RandomForestRegressor(n_estimators=n_est, random_state=seed)\n",
    "rf.fit(X_train.values, y_train.values)\n",
    "\n",
    "with open(output, \"wb\") as fd:\n",
    "    pickle.dump(rf, fd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "7a2a90fe-aafe-432e-802e-74d2d1e0e936",
   "metadata": {},
   "outputs": [],
   "source": [
    "!dvc add models/rf_model.pkl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a5615068-5e5b-4105-b8e0-7716bcca0324",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "To track the changes with git, run:\n",
      "\n",
      "\tgit add models/rf_model.pkl.dvc\n",
      "3 files pushed\n"
     ]
    }
   ],
   "source": [
    "!dvc push"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "5ebc1150-8f20-4d73-9671-0c5dc88a4527",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[master 393cd55] Training 1 completed\n",
      " 2 files changed, 36 insertions(+), 92 deletions(-)\n",
      " create mode 100644 models/rf_model.pkl.dvc\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "To github.com:ramonpzg/pycon-apac21-pipelines.git\n",
      "   c1ffbd5..393cd55  master -> master\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "git add .\n",
    "git commit -m \"Training 1 completed\"\n",
    "git push"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83c90d0a-28c7-4038-bda2-8968cb0d119d",
   "metadata": {},
   "source": [
    "## 6. Model Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6247e0a-4c58-4f72-b0ac-e91387088c2e",
   "metadata": {},
   "source": [
    "Model evaluation is a crucial part of training ML models, and it is important that we pick useful metrics that can indicate to us how well our model is perfoming, or expecting to perform, when presented with unseen data.\n",
    "\n",
    "The metrics we'll use are Mean Absolute Error, Root Mean Squared Error, and $R^2$.\n",
    "\n",
    "- Mean Absolute Error - \"is a measure of errors between paired observations expressing the same phenomenon. Examples of Y versus X include comparisons of predicted versus observed, subsequent time versus initial time, and one technique of measurement versus an alternative technique of measurement.\" ~ [Wikipedia](https://en.wikipedia.org/wiki/Mean_absolute_error)\n",
    "- Root Mean Squared Error - \"is a frequently used measure of the differences between values (sample or population values) predicted by a model or an estimator and the values observed. The RMSD serves to aggregate the magnitudes of the errors in predictions for various data points into a single measure of predictive power. RMSD is a measure of accuracy, to compare forecasting errors of different models for a particular dataset and not between datasets, as it is scale-dependent.\" ~ [Wikipedia](https://en.wikipedia.org/wiki/Root-mean-square_deviation)\n",
    "- $R^2$ - \"In statistics, the coefficient of determination, also spelt co√´fficient, denoted $R^2$ or r2 and pronounced \"R squared\", is the proportion of the variation in the dependent variable that is predictable from the independent variable(s).\" ~ [Wikipedia](https://en.wikipedia.org/wiki/Coefficient_of_determination)\n",
    "\n",
    "We'll start by loading our model and our test set, predict the test set and compare such predictions with the ground truth. After we compute the metrics above, we want to save them to a JSON file for further use and comparison using dvc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "0fa405bd-8ed9-450c-ae91-509a3fe94b07",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.metrics as metrics, json, numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "a5ca84a5-b201-437c-ba66-57ad1fe1a291",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join('models', 'rf_model.pkl'), \"rb\") as fd:\n",
    "    model = pickle.load(fd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "3f8fc6af-f810-47cf-b445-08cdb1d0dcaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = pd.read_csv(os.path.join('data', '03_part', 'processed', 'test.csv'))\n",
    "y_test = X_test.pop('rented_bike_count')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "78e95864-5e5b-4593-8b29-c7839576b50e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1003.81, 1035.99, 1030.2 , 1084.73, 1383.08, 1795.49, 2581.49,\n",
       "       2256.15, 1960.51, 1900.05])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions = model.predict(X_test.values)\n",
    "predictions[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "671c9bae-07e8-4b0d-8dd1-2df5c27ba266",
   "metadata": {},
   "outputs": [],
   "source": [
    "mae = metrics.mean_absolute_error(y_test.values, predictions)\n",
    "rmse = np.sqrt(metrics.mean_squared_error(y_test.values, predictions))\n",
    "r2_score = model.score(X_test.values, y_test.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "f4827685-55a1-4053-9499-ae5fb73b06f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Absolute Error: 191.92\n",
      "Root Mean Square Error: 290.80\n",
      "R^2: 0.788\n"
     ]
    }
   ],
   "source": [
    "print(f\"Mean Absolute Error: {mae:.2f}\")\n",
    "print(f\"Root Mean Square Error: {rmse:.2f}\")\n",
    "print(f\"R^2: {r2_score:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "5f87ee94-9714-4c39-bbd3-5d1c3e9e47e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join('metrics', 'metrics.json'), \"w\") as fd:\n",
    "    json.dump({\"MAE\": mae, \"RMSE\": rmse, \"R^2\":r2_score}, fd, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e491542c-9ea8-4458-b2c6-9ddc2570feb1",
   "metadata": {},
   "source": [
    "We will save the steps above to a file called `evaluate.py` for futher use later, and we will add our metrics to git and GitHub rather than to our remote s3 bucket. The reason beign that dvc has a special function that allows us to compare the diff of metrics between those in a branch and those in master, and as you'll see soon, this is a very powerful feature of dvc that we certainly want to take advantage of."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "f29884bb-80df-4cfc-b399-03c8eaae3d8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting src/full_pipe/evaluate.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile src/full_pipe/evaluate.py\n",
    "\n",
    "import json, os, pickle, sys, pandas as pd, numpy as np\n",
    "import sklearn.metrics as metrics\n",
    "\n",
    "model_file = sys.argv[1]\n",
    "test_file = os.path.join(sys.argv[2], \"test.csv\")\n",
    "scores_file = os.path.join('metrics', 'metrics.json')\n",
    "\n",
    "with open(model_file, \"rb\") as fd:\n",
    "    model = pickle.load(fd)\n",
    "\n",
    "X_test = pd.read_csv(test_file)\n",
    "y_test = X_test.pop('rented_bike_count')\n",
    "\n",
    "predictions = model.predict(X_test.values)\n",
    "\n",
    "mae = metrics.mean_absolute_error(y_test.values, predictions)\n",
    "rmse = np.sqrt(metrics.mean_squared_error(y_test.values, predictions))\n",
    "r2_score = model.score(X_test.values, y_test.values)\n",
    "\n",
    "with open(scores_file, \"w\") as fd:\n",
    "    json.dump({\"MAE\": mae, \"RMSE\": rmse, \"R^2\":r2_score}, fd, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "277f2431-abeb-4e56-9101-391b254080ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "On branch master\n",
      "Your branch is up to date with 'origin/master'.\n",
      "\n",
      "nothing to commit, working tree clean\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Everything up-to-date\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "git add .\n",
    "git commit -m \"Evaluation completed\"\n",
    "git push"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b42a8c6-2815-4950-bca5-885a2461864f",
   "metadata": {},
   "source": [
    "## 7. DVC Pipelines"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4b1c6f0-aee8-47d3-9399-b4ffdf42a8ea",
   "metadata": {},
   "source": [
    "DVC pipelines is one of the best features offered by dvc. They allow us to create reproducible pipilines containing anything from getting the data to training and evaluating ML models.\n",
    "\n",
    "There are several ways for creating pipelines with dvc and here we'll do so with `dvc run`. `dvc run` starts with the `-n` flag followed by the name we want to give to the step of the pipeline we want to create. Next, we add the `-d` flag to signal dependencies such as the python file we want to run as well as any arguments that such file takes. Next we have the `-o` flag which tells dvc the output expected from such step of the pipeline. For example, this stage would take the `train.csv` and `test.csv` files from the data preparation stage. Lastly, you need to pass the full python call without any flags.\n",
    "\n",
    "After we run our dvc command, dvc creates 2 files, a `dvc.yaml` and a `dvc.lock` file. The former contains the stages dvc will follow for our pipeline, and the latter contains the metadata and other information regarding our pipeline. Once you have a look at the yaml file, you'll probably wonder if you can create such a file manually, the answer is yes. For the `dvc.lock` on the other hand, dvc will take care of that one through the command `dvc repro`, which runs whatever pipeline resides in your `dvc.yaml` file.\n",
    "\n",
    "More information about both can be found in the official documentation site [here](https://dvc.org/doc/command-reference/run).\n",
    "\n",
    "Before we start the stages of our pipeline, let's first remove the files we were already tracking with dvc. Not doing so will result in dvc giving us an error since the tracked files already exist."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "70426e36-e515-453f-988a-88abfb4119a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "dvc remove data/03_part/raw/SeoulBikeData.csv.dvc \\\n",
    "           data/03_part/processed/train.csv.dvc \\\n",
    "           data/03_part/processed/test.csv.dvc \\\n",
    "           models/rf_model.pkl.dvc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "940e8e5d-ba3e-49ab-83ef-08308d982dcd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stage 'get_data' is cached - skipping run, checking out outputs\n",
      "Creating 'dvc.yaml'\n",
      "Adding stage 'get_data' in 'dvc.yaml'\n",
      "Generating lock file 'dvc.lock'\n",
      "Updating lock file 'dvc.lock'\n",
      "\n",
      "To track the changes with git, run:\n",
      "\n",
      "\tgit add data/03_part/raw/.gitignore dvc.lock dvc.yaml\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "dvc run -n get_data \\\n",
    "-d src/full_pipe/get_data.py \\\n",
    "-o data/03_part/raw/SeoulBikeData.csv \\\n",
    "python src/full_pipe/get_data.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "304081fb-b4e7-4f10-8459-c8bba4ab7cd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stage 'prepare' is cached - skipping run, checking out outputs\n",
      "Adding stage 'prepare' in 'dvc.yaml'\n",
      "Updating lock file 'dvc.lock'\n",
      "\n",
      "To track the changes with git, run:\n",
      "\n",
      "\tgit add data/03_part/processed/.gitignore dvc.lock dvc.yaml\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "dvc run -n prepare \\\n",
    "-d src/full_pipe/prepare.py -d data/03_part/raw/SeoulBikeData.csv \\\n",
    "-o data/03_part/processed/train.csv -o data/03_part/processed/test.csv \\\n",
    "python src/full_pipe/prepare.py data/03_part/raw/SeoulBikeData.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "ccee8a35-52f6-449f-a3e4-9b073ee62bf3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stage 'train' is cached - skipping run, checking out outputs\n",
      "Adding stage 'train' in 'dvc.yaml'\n",
      "Updating lock file 'dvc.lock'\n",
      "\n",
      "To track the changes with git, run:\n",
      "\n",
      "\tgit add models/.gitignore dvc.lock dvc.yaml\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "dvc run -n train \\\n",
    "-d src/full_pipe/train.py -d data/03_part/processed/train.csv \\\n",
    "-o models/rf_model.pkl \\\n",
    "python src/full_pipe/train.py data/03_part/processed/train.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "6e5d96a3-aa44-45f5-91db-a8a45ff0cceb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stage 'evaluate' is cached - skipping run, checking out outputs\n",
      "Adding stage 'evaluate' in 'dvc.yaml'\n",
      "Updating lock file 'dvc.lock'\n",
      "\n",
      "To track the changes with git, run:\n",
      "\n",
      "\tgit add dvc.yaml dvc.lock\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "dvc run -n evaluate \\\n",
    "-d src/full_pipe/evaluate.py -d models/rf_model.pkl -d data/03_part/processed \\\n",
    "-M metrics/metrics.json \\\n",
    "python src/full_pipe/evaluate.py models/rf_model.pkl data/03_part/processed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0972b3ad-2ff6-417c-abd9-88b30f8b4987",
   "metadata": {},
   "source": [
    "Notice that in the last part of our pipeline we have the flag `-M`. This flag tells dvc to treat the output of that particular stage as a metric so that we can later use `dvc diff` on it and compare the metrics in the master branch with those in another.\n",
    "\n",
    "Using the `dvc status` will tells us whether there are changes in our pipeline and files or if everthing is up to date."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "11b45e5f-dda1-4dec-8443-bd4b7f313525",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data and pipelines are up to date.                                              \n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!dvc status"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16d093e1-076c-4769-8910-3ccbc8c6ff52",
   "metadata": {},
   "source": [
    "Another cool function of dvc is `dvc dag`, which will show us a graph with the steps in our pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "b0ffb172-1e1d-4928-a956-387944a28ec5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        +----------+      \n",
      "        | get_data |      \n",
      "        +----------+      \n",
      "              *           \n",
      "              *           \n",
      "              *           \n",
      "         +---------+      \n",
      "         | prepare |      \n",
      "         +---------+      \n",
      "         **        **     \n",
      "       **            *    \n",
      "      *               **  \n",
      "+-------+               * \n",
      "| train |             **  \n",
      "+-------+            *    \n",
      "         **        **     \n",
      "           **    **       \n",
      "             *  *         \n",
      "        +----------+      \n",
      "        | evaluate |      \n",
      "        +----------+      \u001b[0m"
     ]
    }
   ],
   "source": [
    "!dvc dag"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18d8a6b0-46ab-40c6-95cb-d42d9829f8dd",
   "metadata": {},
   "source": [
    "In order to re-run our pipeline again with one command, `dvc repro`, and see it in action, let's lemove the `dvc.lock` and the data files, and run `dvc repro` once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "53158be4-a3d7-44eb-8051-326ea10df068",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stage 'get_data' didn't change, skipping                              core\u001b[39m>\n",
      "Stage 'prepare' didn't change, skipping\n",
      "Stage 'train' didn't change, skipping\n",
      "Stage 'evaluate' didn't change, skipping                                        \n",
      "Data and pipelines are up to date.\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!dvc repro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "2155d6d9-8126-4d8b-8a0f-e06e66189628",
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm dvc.lock data/03_part/raw/SeoulBikeData.csv data/03_part/processed/train.csv data/03_part/processed/test.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "80132da7-0a83-42b2-a86c-2dd37b0fa401",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stage 'get_data' is cached - skipping run, checking out outputs       core\u001b[39m>\n",
      "Generating lock file 'dvc.lock'                                                 \n",
      "Updating lock file 'dvc.lock'\n",
      "\n",
      "Stage 'prepare' is cached - skipping run, checking out outputs\n",
      "Updating lock file 'dvc.lock'                                                   \n",
      "\n",
      "Stage 'train' is cached - skipping run, checking out outputs\n",
      "Updating lock file 'dvc.lock'                                                   \n",
      "\n",
      "Stage 'evaluate' is cached - skipping run, checking out outputs                 \n",
      "Updating lock file 'dvc.lock'                                                   \n",
      "\n",
      "To track the changes with git, run:\n",
      "\n",
      "\tgit add dvc.lock\n",
      "Use `dvc push` to send your updates to remote storage.\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!dvc repro"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea935677-d0d7-4d6a-a0b6-da770bbbdcc3",
   "metadata": {},
   "source": [
    "Now that we have learned about dvc pipelines and how to reproduce them, let's check the files that need to be committed and let's push them to GitHub."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "94f4afc9-dd33-47b3-a498-8b07fc7a625e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "On branch master\n",
      "Your branch is up to date with 'origin/master'.\n",
      "\n",
      "Changes not staged for commit:\n",
      "  (use \"git add/rm <file>...\" to update what will be committed)\n",
      "  (use \"git restore <file>...\" to discard changes in working directory)\n",
      "\t\u001b[31mdeleted:    data/03_part/processed/test.csv.dvc\u001b[m\n",
      "\t\u001b[31mdeleted:    data/03_part/processed/train.csv.dvc\u001b[m\n",
      "\t\u001b[31mdeleted:    data/03_part/raw/SeoulBikeData.csv.dvc\u001b[m\n",
      "\t\u001b[31mdeleted:    models/rf_model.pkl.dvc\u001b[m\n",
      "\t\u001b[31mmodified:   notebooks/03_dvc_pipes.ipynb\u001b[m\n",
      "\n",
      "Untracked files:\n",
      "  (use \"git add <file>...\" to include in what will be committed)\n",
      "\t\u001b[31mdvc.lock\u001b[m\n",
      "\t\u001b[31mdvc.yaml\u001b[m\n",
      "\n",
      "no changes added to commit (use \"git add\" and/or \"git commit -a\")\n"
     ]
    }
   ],
   "source": [
    "!git status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "8e64290a-66de-42e7-b2cf-8d83344ccaac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[master 88c6c06] Pipeline Finished\n",
      " 7 files changed, 146 insertions(+), 74 deletions(-)\n",
      " delete mode 100644 data/03_part/processed/test.csv.dvc\n",
      " delete mode 100644 data/03_part/processed/train.csv.dvc\n",
      " delete mode 100644 data/03_part/raw/SeoulBikeData.csv.dvc\n",
      " create mode 100644 dvc.lock\n",
      " create mode 100644 dvc.yaml\n",
      " delete mode 100644 models/rf_model.pkl.dvc\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "To github.com:ramonpzg/pycon-apac21-pipelines.git\n",
      "   393cd55..88c6c06  master -> master\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "git add .\n",
    "git commit -m \"Pipeline Finished\"\n",
    "git push"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "74405837-7685-42eb-a2ca-3e0ada66b0f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "On branch master\n",
      "Your branch is up to date with 'origin/master'.\n",
      "\n",
      "nothing to commit, working tree clean\n"
     ]
    }
   ],
   "source": [
    "!git status"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5cf1b6b-9bbb-4812-a594-4d7ed89a685a",
   "metadata": {},
   "source": [
    "## 9. Experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4618ae71-706e-4e04-9f37-f11ba9261751",
   "metadata": {},
   "source": [
    "We've been working inside our master branch using scikit-learn, and now we want to start experimenting with other tree-based frameworks like XGBoost, LightGBM, and CatBoost using different branches for each experiment. Let's do just that and start by checking out a new branch, adding XGBoost to our train file, and triggering a new run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "3bd70cb5-9d1f-44fa-8473-0bfbdbc38f3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Switched to a new branch 'exp1-xgb'\n"
     ]
    }
   ],
   "source": [
    "!git checkout -b \"exp1-xgb\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "152cfddc-d6c9-47af-b0b6-311c21e49164",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting src/full_pipe/train.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile src/full_pipe/train.py\n",
    "\n",
    "import os, pickle, sys, pandas as pd\n",
    "from xgboost import XGBRFRegressor\n",
    "\n",
    "input_data = sys.argv[1]\n",
    "output = os.path.join('models', 'rf_model.pkl')\n",
    "seed, n_est = 42, 100\n",
    "\n",
    "X_train = pd.read_csv(input_data)\n",
    "y_train = X_train.pop('rented_bike_count')\n",
    "\n",
    "rf = XGBRFRegressor(n_estimators=n_est, seed=seed)\n",
    "rf.fit(X_train.values, y_train.values)\n",
    "\n",
    "with open(output, \"wb\") as fd: pickle.dump(rf, fd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "302f4bc2-a664-44a5-9d08-f96bd60ec9ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stage 'get_data' didn't change, skipping                              core\u001b[39m>\n",
      "Stage 'prepare' didn't change, skipping\n",
      "Running stage 'train':\n",
      "> python src/full_pipe/train.py data/03_part/processed/train.csv\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ramonperez/Tresors/datascience/tutorials/pycon_apac21/src/full_pipe/train.py\", line 3, in <module>\n",
      "    from xgboost import XGBRFRegressor\n",
      "ModuleNotFoundError: No module named 'xgboost'\n",
      "\u001b[31mERROR\u001b[39m: failed to reproduce 'dvc.yaml': failed to run: python src/full_pipe/train.py data/03_part/processed/train.csv, exited with 1\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!dvc repro"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e90c2fa0-7ccf-4253-8fba-c9ce63b3ea2f",
   "metadata": {},
   "source": [
    "Note that in order for GitHub to now we have been working in a different branch, we need to use the `git push --set-upstream origin exp1-xgb` command. Otherwise, we'll get an error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "e8170f9d-7471-4867-8537-fce3a8ba26b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[exp1-xgb fb6dcf7] Testing XGBoost\n",
      " 2 files changed, 48 insertions(+), 253 deletions(-)\n",
      " create mode 100644 src/train.py\n",
      "Branch 'exp1-xgb' set up to track remote branch 'exp1-xgb' from 'origin'.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "remote: \n",
      "remote: Create a pull request for 'exp1-xgb' on GitHub by visiting:        \n",
      "remote:      https://github.com/ramonpzg/pycon-apac21-pipelines/pull/new/exp1-xgb        \n",
      "remote: \n",
      "To github.com:ramonpzg/pycon-apac21-pipelines.git\n",
      " * [new branch]      exp1-xgb -> exp1-xgb\n",
      "Everything up-to-date\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "git add .\n",
    "git commit -m \"Testing XGBoost\"\n",
    "git push --set-upstream origin exp1-xgb\n",
    "git push"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "f5fd561a-a6c9-449e-8f54-f0afb899d676",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| Path   | Metric   | Old   | New   | Change   |                      core\u001b[39m>\n",
      "|--------|----------|-------|-------|----------|\n",
      "\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!dvc metrics diff --show-md master"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0a4c1a2-3990-4569-89a0-21af8fdf3bca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "87331801-a13d-4e49-a6e3-94118516e4f5",
   "metadata": {},
   "source": [
    "Following the same process as the one from the previous section, we can go to the **Actions** tab and have a look at our XGBoost run.\n",
    "![actions](../images/pipe_xgb.png)\n",
    "\n",
    "In addition, thanks to the `dvc metrics diff --show-md master` command in our CI/CD pipeline, we can now look at the difference in metrics between our current branch and the master one.\n",
    "![actions](../images/xgb_metrics.png)\n",
    "\n",
    "It seems that a plain baseline XGB model performs a bit worse than our sklearn one. Let's try LightGBM and see how it does. We'll move to a new branch, update the `train.py` file, and trigger the run on git push."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbc447c7-1ee3-4b79-b1e1-c1a9a9a990fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "!git checkout -b \"exp2-lgbm\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcaa78b4-aa5e-4a32-90ce-ba5db417475e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile src/train.py\n",
    "\n",
    "import os, pickle, sys, pandas as pd\n",
    "from lightgbm import LGBMRegressor\n",
    "\n",
    "input_data = sys.argv[1]\n",
    "output = os.path.join('models', 'rf_model.pkl')\n",
    "seed, n_est = 42, 100\n",
    "\n",
    "X_train = pd.read_csv(input_data)\n",
    "y_train = X_train.pop('rented_bike_count')\n",
    "\n",
    "rf = LGBMRegressor(n_estimators=n_est, random_state=seed)\n",
    "rf.fit(X_train.values, y_train.values)\n",
    "\n",
    "with open(output, \"wb\") as fd: pickle.dump(rf, fd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1031445f-3935-461b-b5b1-2c62599f2fc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "git add .\n",
    "git commit -m \"Testing LightGBM\"\n",
    "git push --set-upstream origin exp2-lgbm\n",
    "git push"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "251c8b37-6d23-4c92-a5e8-322a4cbf1506",
   "metadata": {},
   "source": [
    "![actions](../images/lgbm_metrics.png)\n",
    "The base implementation of LightGBM seems to have performed quite well. Let's try out CatBoost now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96db5255-9607-42cb-87a0-c21e02428de5",
   "metadata": {},
   "outputs": [],
   "source": [
    "!git checkout -b \"exp3-cat\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cee8833d-2086-4769-8ee5-7ae4538e28c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile src/train.py\n",
    "\n",
    "import os, pickle, sys, pandas as pd\n",
    "from catboost import CatBoostRegressor\n",
    "\n",
    "input_data = sys.argv[1]\n",
    "output = os.path.join('models', 'rf_model.pkl')\n",
    "seed, n_est = 42, 100\n",
    "\n",
    "X_train = pd.read_csv(input_data)\n",
    "y_train = X_train.pop('rented_bike_count')\n",
    "\n",
    "rf = CatBoostRegressor(n_estimators=n_est, random_state=seed)\n",
    "rf.fit(X_train.values, y_train.values)\n",
    "\n",
    "with open(output, \"wb\") as fd: pickle.dump(rf, fd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ab99a16-b2e7-4045-93fd-39a0cf5e3cab",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "git add .\n",
    "git commit -m \"Testing CatBoost\"\n",
    "git push --set-upstream origin exp3-cat\n",
    "git push"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "215aaaf3-e6d1-450a-ac79-8c82a3c9f42e",
   "metadata": {},
   "source": [
    "![actions](../images/cat_metrics.png)\n",
    "\n",
    "Surprisingly, CatBoost's MAE performed a bit worse than LightGBM but RMSE performed much better.\n",
    "\n",
    "You can go to your Actions tab again and see a recap of all of your runs.\n",
    "![exps](../images/exps.png)\n",
    "\n",
    "We have a good candidate with CatBoost and we should merge this branch with master and start tunning our model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6e4b759-bac7-4503-94c9-9eaaaf5c5648",
   "metadata": {},
   "source": [
    "## 10. Merging our Changes - PRs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bff1fa0-20ca-4e7b-81a4-94f79c79c7f4",
   "metadata": {},
   "source": [
    "![empspr](../images/compare_pr.png)\n",
    "If you go back to the main page of our repo, you'll notice that GitHub has added a **Compare & pull request** option for each the three experiments. This is a nice shortcut to help us pick the one we liked best and add it to our main project's branch, master.\n",
    "\n",
    "So what is a pull request anyways? \"A PR provides a user-friendly web interface for discussing proposed changes before integrating them into the official project.\" ~ [Atlassian](https://www.atlassian.com/git/tutorials/making-a-pull-request)\n",
    "\n",
    "Let's merge our experiment branch with our master branch.\n",
    "\n",
    "1. Click on the **Compare & pull request** for exp3-cat.\n",
    "2. Compare the changes.\n",
    "![comp](../images/compare_pr.png)\n",
    "3. Check out the report again.\n",
    "![rep](../images/report_bottom.png)\n",
    "4. Open a pull request with your details on why it should go to master.\n",
    "![rep](../images/pr_mess.png)\n",
    "5. Once reviewed, write a comment and merge the pull request.\n",
    "![rep](../images/awesome_cat.png)\n",
    "6. Lastly, we need to make sure our local env is up to date and once it is, we can switch to the master branch and work from there again. Run a `git pull` and a `dvc pull`.\n",
    "![list_prs](../images/last_of_pr.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ec3658f-fd98-4365-8bb1-ba6239e77031",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "git pull\n",
    "dvc pull"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1aab025a-ed5e-4190-959c-77734b68e4cb",
   "metadata": {},
   "source": [
    "## 11. Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9e638f6-61ea-4371-bbca-203450647d7a",
   "metadata": {},
   "source": [
    "As you have seen throughout the tutorial\n",
    "\n",
    "DVC helps us track our data, models, and metrics, and it also allows us to create pipelines for getting, preparing, and modeling data. In contrast, CML allows us to continuously deliver ML models through its CI/CD configuration alongside dvc. CML makes it easier to experiment and deploy models in a production environment.\n",
    "\n",
    "DVC and CML are making possible what git alone can't do for the machine learning community, and the do this by enhancing git in the parts where it's lacking. Both tools should be in every data scientist and ML Engineer's toolkit. Enough said!\n",
    "\n",
    "![enough](https://media.giphy.com/media/mVJ5xyiYkC3Vm/giphy.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0ce9603-232b-49da-9df5-96a113fae284",
   "metadata": {},
   "source": [
    "## 12. Blind Spots and Future Work"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "057a8d49-405f-441f-910d-4f9ff78cc391",
   "metadata": {},
   "source": [
    "**Blind Spots**\n",
    "- We could have fine tuned our base model even further and make better comparisons with the other frameworks.\n",
    "- We could have conducted more feature engineering.\n",
    "- We could have selected the best features only based on feature importance.\n",
    "- We could have done a bit more analysis of the data.\n",
    "- We could have taken out the second dummy or our categorical variables. For example, there is no need to have Holiday and No Holiday as variables in our dataset.\n",
    "\n",
    "\n",
    "**Future Work**\n",
    "- If the data will be provided in the same formate we received it, then we need an easier transformation pipeline for the date, column names, and dummies.\n",
    "- We could add the analytical tool, e.g. our dashboard, to the master branch and work with the models solely through branches."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1a08954-2792-4f3e-a787-100500ecd109",
   "metadata": {},
   "source": [
    "## 13. Resources"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "720f2d19-a43e-40b0-af52-0ced35baee4b",
   "metadata": {},
   "source": [
    "Here are a few additional resources to dive deeped into some of the tools discussed above.\n",
    "- [DVC Get Started](https://dvc.org/doc/start)\n",
    "- [DVC Use Cases](https://dvc.org/doc/use-cases)\n",
    "- [CML Get Started](https://cml.dev/doc/start)\n",
    "- [CatBoost Tutorial](https://catboost.ai/en/docs/concepts/tutorials)\n",
    "- [Git](https://realpython.com/python-git-github-intro)\n",
    "- [Pull Requests](https://www.atlassian.com/git/tutorials/making-a-pull-request)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
