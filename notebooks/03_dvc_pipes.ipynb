{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "182c06ef-fcda-4850-9dc0-4c15af093554",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 04 Full Pipelines with DVC"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d09c1d0-1e72-4d51-a171-976c7703a3c1",
   "metadata": {},
   "source": [
    "> â€œThe most powerful tool we have as developers is automation.â€ ~ Scott Hanselman\n",
    "\n",
    "![dvc_cml](https://blog.codecentric.de/files/2019/03/logo-owl-readme.png)\n",
    "\n",
    "Image source: [dvc.org](https://dvc.org/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdc92b8c-f5c2-4de9-8e18-2284237fb459",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Synopsis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec361a60-5d9b-43b3-8587-ef5c9038fb66",
   "metadata": {},
   "source": [
    "If you have ever created machine learning pipelines of any kind and wondered if it would be possible to combine your data, feature engineering, training and model saving in one single place where you could experiment to your heart's content, ðŸ³, look no further as it possible with DVC and you will learn about it here. The aim of this tutorial is to show you a step-by-step process for creating reproducible pipelines to gather, clean, train, evaluate, and track machine learning models with DVC (data version control), and other tools in the Data Science stack. By the end of this tutorial, you will have the tools to create your own reproducible pipelines and keep track not only of your code but also of your data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb175b56-595e-4403-ba83-5a4c636a9b43",
   "metadata": {},
   "source": [
    "## Learning Outcomes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8d19268-51ac-42af-910e-bf6e0edafe48",
   "metadata": {},
   "source": [
    "By the end of the tutorial you would have learned\n",
    "1. a bit of git for keeping track of your code\n",
    "2. a bit of dvc to track your different datasets and machine learning models\n",
    "3. a bit of ML pipelines\n",
    "4. a bit of ML modeling and some python tools for it"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68517f95-bc69-4c75-827c-ce7b91652e62",
   "metadata": {},
   "source": [
    "## Table of Contents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab335fb6-23d9-49df-b679-15571fadae58",
   "metadata": {},
   "source": [
    "1. [Scenario](#1.-Scenario)\n",
    "2. [The Tools](#2.-The-Tools)\n",
    "3. [Project Structure](#3.-Environment-Set-Up)\n",
    "4. [The Data](#4.-The-Data)\n",
    "    - [Getting the Data](#4.1-Getting-the-Data)\n",
    "    - [Preparing the Data](#4.2-Preparing-the-Data)\n",
    "5. [Training our First Model](#5.-Training-our-First-Model)\n",
    "6. [Model Evaluation](#6.-Model-Evaluation)\n",
    "7. [DVC Pipelines](#7.-DVC-Pipelines)\n",
    "8. [Experiments](#8.-Experiments)\n",
    "9. [Merging our Changes - PRs](#9.-Merging-our-Changes---PRs)\n",
    "10. [Summary](#10.-Summary)\n",
    "11. [Blind Spots and Future Work](#11.-Blind-Spots-and-Future-Work)\n",
    "12. [Resources](#12.-Resources)\n",
    "\n",
    "**NB:** this tutorial was built in a Linux machine and some terminal commands might only be available in \\*NIX-based systems."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fea842b9-e9ef-49f5-93be-1580e8f365c3",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 1. Scenario"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b394fb1e-3bf9-4525-97b6-82898cdbb46e",
   "metadata": {},
   "source": [
    "![bikes_seoul](https://img.koreatimes.co.kr/upload/newsV2/images/202103/3e9b5801c43048eca31b3309176c8da9.jpg)\n",
    "\n",
    "Imagine you work at a data analytics consultancy called **Beautiful Analytics**, and that your boss comes to you with a new challenge for you, to create a machine learning model to predict the amount of bikes neeeded at any given hour of the day in Seoul, South Korea. You don't know anything about bicycle rental systems but you're excited to take on the challenge and accept it with pleasure.\n",
    "\n",
    "The challenge was presented to your boss by the South Korean government, and what they are hoping to get later on is an in-house analytical product that anyone can use to figure out the amount of rental bicycles needed at any given time and at different locations in the city of Seoul. You will tackle the predictive modeling part while the rest of team will work on the application and the geospatial part of the task.\n",
    "\n",
    "Lastly, Beautiful Analytics has been improving their data science capabilities and would like for every project to use data and model version control tools, this means you will be using dvc and other cool tools for the first time for this task. Let's go over the tooling in the next section."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc372370-9f64-4fac-a64e-184513a735bf",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 2. The Tools"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28af17fe-063c-44a3-bb9a-7dcf7ac4290a",
   "metadata": {},
   "source": [
    "Here are some of the tools that we will be using.\n",
    "\n",
    "- [DVC](https://dvc.org/) - \"Data Version Control, or DVC, is a data and ML experiment management tool that takes advantage of the existing engineering toolset that you're already familiar with (Git, CI/CD, etc.).\"\n",
    "- [NumPy](https://numpy.org/) - \"It is a Python library that provides a multidimensional array object, various derived objects (such as masked arrays and matrices), and an assortment of routines for fast operations on arrays, including mathematical, logical, shape manipulation, sorting, selecting, I/O, discrete Fourier transforms, basic linear algebra, basic statistical operations, random simulation and much more.\"\n",
    "- [pandas](https://pandas.pydata.org/) - \"is a fast, powerful, flexible and easy to use open source data analysis and manipulation tool, built on top of the Python programming language.\"\n",
    "- [scikit-learn](https://scikit-learn.org/stable/index.html) - \"is an open source machine learning library that supports supervised and unsupervised learning. It also provides various tools for model fitting, data preprocessing, model selection and evaluation, and many other utilities.\"\n",
    "- [XGBoost](https://xgboost.readthedocs.io/en/latest/index.html) - \"is an optimized distributed gradient boosting library designed to be highly efficient, flexible and portable. It implements machine learning algorithms under the Gradient Boosting framework. XGBoost provides a parallel tree boosting (also known as GBDT, GBM) that solve many data science problems in a fast and accurate way.\"\n",
    "- [LightGBM](https://lightgbm.readthedocs.io/en/latest/index.html) - \"is a gradient boosting framework that uses tree based learning algorithms. It is designed to be distributed and efficient with the following advantages: Faster training speed and higher efficiency, lower memory usage, better accuracy, support of parallel, distributed, and GPU learning, and capable of handling large-scale data.\"\n",
    "- [CatBoost](https://catboost.ai/en/docs/) - \"CatBoost is a machine learning algorithm that uses gradient boosting on decision trees. It is available as an open source library.\"\n",
    "- [Git](https://git-scm.com/) - \"Git is a free and open source distributed version control system designed to handle everything from small to very large projects with speed and efficiency.\"\n",
    "\n",
    "Let's now set up our development environment and get started with our project.\n",
    "\n",
    "**NB:** the definitions above have been taken directly from the tools' respective websites."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a2aeb89-667c-4da3-ab20-a1e99c2f9357",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 3. Project Structure"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dacc120-e6c8-416e-8ea3-b7d3b51224c0",
   "metadata": {},
   "source": [
    "Usually we want to work with a directory with a somewhat similar structure to the one below but for the purpose of this tutorial, the set up will be slightly different and everything will happen from within this notebook.\n",
    "\n",
    "```bash\n",
    ".\n",
    "â”œâ”€â”€ data\n",
    "â”‚Â Â  â”œâ”€â”€ processed\n",
    "â”‚Â Â  â”‚Â Â  â”œâ”€â”€ test\n",
    "â”‚Â Â  â”‚Â Â  â””â”€â”€ train\n",
    "â”‚Â Â  â””â”€â”€ raw\n",
    "â”œâ”€â”€ metrics\n",
    "â”œâ”€â”€ models\n",
    "â”œâ”€â”€ notebooks\n",
    "â”œâ”€â”€ src\n",
    "â””â”€â”€ README.md\n",
    "```\n",
    "\n",
    "\n",
    "Let's now initialize our git and dvc repositories with the following commands.\n",
    "\n",
    "```bash\n",
    "git init\n",
    "\n",
    "dvc init\n",
    "```\n",
    "\n",
    "You should see the following output for dvc.\n",
    "\n",
    "![gitdvc](../images/git_dvc.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5439f776-c75a-4a00-8e81-92ce7506fa75",
   "metadata": {},
   "source": [
    "If you are in binder or a different environment, make sure you run the following cell first for a fresh start."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb5243e0-f99c-4a20-b073-5a47ff20108f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !rm -r .git .dvc dvc.lock dvc.yaml metrics/* models/* .dvcignore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b529cc80-17b8-480c-b9d9-e06b19cadd87",
   "metadata": {},
   "outputs": [],
   "source": [
    "!git init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "733328b9-7270-45ec-bf17-772c268a6429",
   "metadata": {},
   "outputs": [],
   "source": [
    "!dvc init"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a09ed3dc-4b3d-442a-aa25-f68619e7b1a2",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 4. The Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4208ae1-2637-4cb2-a7d8-b75b6d5f929d",
   "metadata": {},
   "source": [
    "Following the described scenario above, a sample data was donated to the UCI ML Repository on 2020-03-01 for a regression task. It contains information regarding the amount of bikes available per hour between 2017 and 2018.\n",
    "\n",
    "![bikes_uci](../images/uci_bikes.png)\n",
    "\n",
    "Here are the variables found in the dataset.\n",
    "\n",
    "- `Date` - year-month-day\n",
    "- `Rented Bike count` - Count of bikes rented at each hour\n",
    "- `Hour` - Hour of the day\n",
    "- `Temperature`-Temperature in Celsius\n",
    "- `Humidity` - %\n",
    "- `Windspeed` - m/s\n",
    "- `Visibility` - 10m\n",
    "- `Dew point temperature` - Celsius\n",
    "- `Solar radiation` - MJ/m2\n",
    "- `Rainfall` - mm\n",
    "- `Snowfall` - cm\n",
    "- `Seasons` - Winter, Spring, Summer, and Autumn\n",
    "- `Holiday` - Holiday and No holiday\n",
    "- `Functional Day` - NoFunc (Non Functional Hours), Fun (Functional hours)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01f8fd2c-741e-4656-8f8a-62b3585f96e6",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 4.1 Getting the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17facbd6-b65e-41ff-9b42-e8ebc39a9704",
   "metadata": {},
   "source": [
    "We will use the `urllib.request` and the `os` libraries to download the data and set up our desired path for it, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0061face-8fbd-4a62-b477-fa7472520bf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request, os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91c0632b-3224-4f5d-a328-4471799a0ca8",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d136262-263d-47d6-8bac-3c070001eb22",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(\"..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cbc15da-8598-431a-ad3e-87217fe64584",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a4e0fac-7166-4b69-8614-2958cd95c79b",
   "metadata": {},
   "source": [
    "The dataset can be downloaded from the URL below and we will give it the filename, `SeoulBikeData.csv`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e017317-2ef4-4756-95aa-9f5c1f8bbe50",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://archive.ics.uci.edu/ml/machine-learning-databases/00560/SeoulBikeData.csv'\n",
    "path_and_filename = os.path.join('data', '03_part', 'raw', 'SeoulBikeData.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8122ea37-6f1e-440d-b868-559e486a2875",
   "metadata": {},
   "outputs": [],
   "source": [
    "urllib.request.urlretrieve(url, path_and_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6ac07ff-3bd0-4bb1-8a99-1792a211b279",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.read_csv(path_and_filename, encoding='iso-8859-1').head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fbb66c6-b3dc-4905-a414-86a7a99bc8c6",
   "metadata": {},
   "source": [
    "Because we want to be able to create a pipeline later on, we will export the few lines of code above as a python script called `get_data.py` for later use. We will put every python script we want in our pipeline inside the `src` directory. Get used to this pattern. :)\n",
    "\n",
    "Also, it is good practice to make sure the directories we are using always exist, so we will add an additional if-else statement to search and/or create it if it does not exist.\n",
    "\n",
    "The command `%%writefile` below is a magic function and these are special functions of our ipython interpreter. The one we are using allows us to write anything in that cell to a file. Others like the `%%bash`, as we will soon see, make the entire cell a bash executable cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89532f82-87b4-4bcb-8381-d228f7425555",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile src/full_pipe/get_data.py\n",
    "\n",
    "import urllib.request, os\n",
    "\n",
    "url = 'https://archive.ics.uci.edu/ml/machine-learning-databases/00560/SeoulBikeData.csv'\n",
    "path = os.path.join('data', '03_part', 'raw')\n",
    "filename = 'SeoulBikeData.csv'\n",
    "\n",
    "if not os.path.exists(path): os.makedirs(path)\n",
    "        \n",
    "urllib.request.urlretrieve(url, os.path.join(path, filename))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36a7112e-6850-4b89-9495-479f1d238d6b",
   "metadata": {},
   "source": [
    "Now that we have our dataset, let's go ahead and add it to our remote storage and start keeping track of the changes that we make to it with dvc. We will use Google Drive as our primary storage tool for the tutorial but feel free to use the option that best suits your needs and experience from the [dvc website](https://dvc.org/doc/command-reference/remote/add). Here are the step to follow.\n",
    "\n",
    "1. Log into your Google Drive with your gmail or create a gmail account and then log into your gdrive.\n",
    "2. Create a folder for this tutorial and copy the alphanumeric string after the last `/`.\n",
    "![tuto](../images/gdrive1.png)\n",
    "3. Add your remote storage with `dvc remote add -d bikestorage gdrive://your_string`\n",
    "4. You will be redirected to a page so that you can allow `dvc` to access such folder. Select the boxes and confirm.\n",
    "5. Add the data/models/files you want to track with `dvc add data/03_part/raw/SeoulBikeData.csv`\n",
    "6. From the command line run `dvc push` and this will ask you from one last authentication step\n",
    "    - Copy and paste the link provided in a browser tab\n",
    "    - copy the code provided and paste it on the terminal\n",
    "    ![ipush](../images/dvc_push.png)\n",
    "\n",
    "**You should be good to go** so now let's add our bucket to our dvc repo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c766232-1858-4909-9c9f-fb050fefce60",
   "metadata": {},
   "outputs": [],
   "source": [
    "!dvc remote add -d bikestorage gdrive://1hhbxsGSxrVRcJaxI8_cZ9wroJzo3DsVy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5471685-f7a9-4ad3-88d6-269fc5ac6acd",
   "metadata": {},
   "source": [
    "In the first line above the `-d` stands for default and `bikestorage` is the name we have decided on for our bucket. The last piece is the url that directs dvc to our gdrive. You can find out more about the `remote` command through the official docummentation [here](https://dvc.org/doc/command-reference/remote).\n",
    "\n",
    "Now let's start tracking our data and make sure our remote storage is fully connected to our local storage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9aa0be8-c133-49bd-9880-f5b88eef02e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "!dvc add data/03_part/raw/SeoulBikeData.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd8b3510-cf29-4da6-9c87-130af1cafc5e",
   "metadata": {},
   "source": [
    "You will need to run `dvc push` from the terminal.\n",
    "\n",
    "![fullbucket](../images/file_up.png)\n",
    "\n",
    "DVC uses special names to keep track of files, so there's no need to try and figure out what the above name means. Everything in our bucket can always be accessed through dvc.\n",
    "\n",
    "Lastly, we'll commit our changes to our git repo after making sure we add the two files created by dvc, `data/03_part/raw/.gitignore` and `data/03_part/raw/SeoulBikeData.csv.dvc`. What dvc is doing is tracking some information about our dataset through git, hence the files `...Data.csv.dvc` and `.gitignore` with the actual data file goes into git while the actual data goes to our remote storage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e465248-b384-4a7d-9a4b-8a5e19350abc",
   "metadata": {},
   "outputs": [],
   "source": [
    "!git add data/03_part/raw/SeoulBikeData.csv.dvc data/03_part/raw/.gitignore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40eeca7e-84f8-410d-adcc-8682bddb27d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "!git status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09d85c8d-97c7-45e0-af89-5211823d01f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "git commit -m \"Start Tracking Data\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcb24e02-eb5b-40ed-9455-e52869833a5a",
   "metadata": {},
   "source": [
    "Before we push any changes to GitHub, make sure you create your repository as shown here, and then connect your local and remote repo with the commands below.\n",
    "![reposetup](../images/repo_setup.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b874731c-a946-4ee4-a77f-10454647a172",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "# git remote add origin https://github.com/ramonpzg/your_repo.git\n",
    "git push -u origin master"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c260fcd-a8ff-4fae-968c-d27e6d6bac7a",
   "metadata": {},
   "source": [
    "### 4.2 Preparing the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97c08628-20c4-4459-8e96-45a04de65256",
   "metadata": {},
   "source": [
    "The following steps should feel familiar to us, we want to separate the date variable into its components, create dummy variables for the categorical features, normalize the columns so that they only contain alphanumerical characters with underscores instead of spaces, and finally split the data into train and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d801a38d-bb07-4811-8d7d-464a2f0b7e02",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = pd.read_csv(os.path.join('data', '03_part', 'raw', 'SeoulBikeData.csv'), encoding='iso-8859-1')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab585257-f164-4dc4-89a5-5cf94a6fb1d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['Date'] = pd.to_datetime(data['Date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47fbada1-711d-4bad-8fb8-3cfb1096afe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.sort_values(['Date', 'Hour'], inplace=True)\n",
    "data[\"Year\"] = data['Date'].dt.year\n",
    "data[\"Month\"] = data['Date'].dt.month\n",
    "data[\"Week\"] = data['Date'].dt.isocalendar().week\n",
    "data[\"Day\"] = data['Date'].dt.day\n",
    "data[\"Dayofweek\"] = data['Date'].dt.dayofweek\n",
    "data[\"Dayofyear\"] = data['Date'].dt.dayofyear\n",
    "data[\"Is_month_end\"] = data['Date'].dt.is_month_end\n",
    "data[\"Is_month_start\"] = data['Date'].dt.is_month_start\n",
    "data[\"Is_quarter_end\"] = data['Date'].dt.is_quarter_end\n",
    "data[\"Is_quarter_start\"] = data['Date'].dt.is_quarter_start\n",
    "data[\"Is_year_end\"] = data['Date'].dt.is_year_end\n",
    "data[\"Is_year_start\"] = data['Date'].dt.is_year_start\n",
    "data.drop('Date', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3491f53-17c0-4edc-9e22-8e352f08fe78",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.get_dummies(data=data, columns=['Holiday', 'Seasons', 'Functioning Day'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb490f64-7264-4b6b-82a8-f9847b1cb872",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.columns = ['rented_bike_count', 'hour', 'temperature', 'humidity', 'wind_speed', 'visibility', \n",
    "                'dew_point_temperature', 'solar_radiation', 'rainfall', 'snowfall', 'year', \n",
    "                'month', 'week', 'day', 'dayofweek', 'dayofyear', 'is_month_end', 'is_month_start',\n",
    "                'is_quarter_end', 'is_quarter_start', 'is_year_end', 'is_year_start',\n",
    "                'seasons_autumn', 'seasons_winter', 'seasons_summer', 'seasons_spring',\n",
    "                'holiday_yes', 'holiday_no', 'functioning_day_no', 'functioning_day_yes']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fc5eff2-566e-4155-8e78-630dc204c0d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "split = 0.30\n",
    "n_train = int(len(data) - len(data) * split)\n",
    "\n",
    "train_path = os.path.join('data', '03_part', 'processed', 'train.csv')\n",
    "test_path = os.path.join('data', '03_part', 'processed', 'test.csv')\n",
    "\n",
    "data[:n_train].reset_index(drop=True).to_csv(train_path, index=False)\n",
    "data[n_train:].reset_index(drop=True).to_csv(test_path, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5da31f9-2d67-4a29-96b1-ded03e2481b0",
   "metadata": {},
   "source": [
    "Using the same commands as before, let's keep track of our new datasets with dvc and push the changes to our gdrive. Also, we'll create a file called `prepared.py` for later use in our pipelines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ce3a228-463e-471d-9ab4-e7159c452412",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "dvc add data/03_part/processed/train.csv data/03_part/processed/test.csv\n",
    "dvc push"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ff7b8c7-7d43-4394-b7b4-22d97ce10b82",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile src/full_pipe/prepare.py\n",
    "\n",
    "import pandas as pd\n",
    "import os, sys\n",
    "\n",
    "split = 0.30\n",
    "\n",
    "raw_data_path = sys.argv[1]\n",
    "train_path = os.path.join('data', '03_part', 'processed', 'train.csv')\n",
    "test_path = os.path.join('data', '03_part', 'processed', 'test.csv')\n",
    "\n",
    "# read the data\n",
    "data = pd.read_csv(raw_data_path, encoding='iso-8859-1')\n",
    "\n",
    "# add date vars\n",
    "data['Date'] = pd.to_datetime(data['Date'])\n",
    "data.sort_values(['Date', 'Hour'], inplace=True)\n",
    "data[\"Year\"] = data['Date'].dt.year\n",
    "data[\"Month\"] = data['Date'].dt.month\n",
    "data[\"Week\"] = data['Date'].dt.isocalendar().week\n",
    "data[\"Day\"] = data['Date'].dt.day\n",
    "data[\"Dayofweek\"] = data['Date'].dt.dayofweek\n",
    "data[\"Dayofyear\"] = data['Date'].dt.dayofyear\n",
    "data[\"Is_month_end\"] = data['Date'].dt.is_month_end\n",
    "data[\"Is_month_start\"] = data['Date'].dt.is_month_start\n",
    "data[\"Is_quarter_end\"] = data['Date'].dt.is_quarter_end\n",
    "data[\"Is_quarter_start\"] = data['Date'].dt.is_quarter_start\n",
    "data[\"Is_year_end\"] = data['Date'].dt.is_year_end\n",
    "data[\"Is_year_start\"] = data['Date'].dt.is_year_start\n",
    "data.drop('Date', axis=1, inplace=True)\n",
    "\n",
    "# add dummies\n",
    "data = pd.get_dummies(data=data, columns=['Holiday', 'Seasons', 'Functioning Day'])\n",
    "\n",
    "# Normalize columns\n",
    "data.columns = ['rented_bike_count', 'hour', 'temperature', 'humidity', 'wind_speed', 'visibility', \n",
    "                'dew_point_temperature', 'solar_radiation', 'rainfall', 'snowfall', 'year', \n",
    "                'month', 'week', 'day', 'dayofweek', 'dayofyear', 'is_month_end', 'is_month_start',\n",
    "                'is_quarter_end', 'is_quarter_start', 'is_year_end', 'is_year_start',\n",
    "                'seasons_autumn', 'seasons_winter', 'seasons_summer', 'seasons_spring',\n",
    "                'holiday_yes', 'holiday_no', 'functioning_day_no', 'functioning_day_yes']\n",
    "\n",
    "n_train = int(len(data) - len(data) * split)\n",
    "data[:n_train].reset_index(drop=True).to_csv(train_path, index=False)\n",
    "data[n_train:].reset_index(drop=True).to_csv(test_path, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f63708ba-3d1f-440a-a9d3-6d31b1178555",
   "metadata": {},
   "source": [
    "Now we are ready to commit all of our changes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f468cb08-e2b3-456b-8a99-7491d1fe6bba",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "git add .\n",
    "git commit -m \"Preparation stage completed\"\n",
    "git push"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69fb0768-f72c-4a52-bf5f-366dd7cb9d77",
   "metadata": {},
   "source": [
    "## 5. Training our First Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad274b95-b346-4d8f-8a0f-6626ae45c62a",
   "metadata": {},
   "source": [
    "We want to create a model that predicts how many bikes will be needed at any given hour and on any given date in the future of the city of Seoul. Since the number of bicycles available for rent is a continuous number, this is a regression problem and what a better tool to use for regression problems that Random Forests.\n",
    "\n",
    "![rfs](https://media.makeameme.org/created/its-easy-just-5bd65c.jpg)\n",
    "\n",
    "What are Random Forests anyways?\n",
    "\n",
    "> \"Random forests or random decision forests are an ensemble learning method for classification, regression and other tasks that operates by constructing a multitude of decision trees at training time. For classification tasks, the output of the random forest is the class selected by most trees. For regression tasks, the mean or average prediction of the individual trees is returned. Random decision forests correct for decision trees' habit of overfitting to their training set.\" ~ [Wikipedia](https://en.wikipedia.org/wiki/Random_forest)\n",
    "\n",
    "We want to start with a baseline model, evaluate it, and then fine tune either the implementation that we picked, in this case the scikit-learn's one or, as we'll see in a later section, an implementation from another framework.\n",
    "\n",
    "After we train our sklearn model, we want to serialize (or pickle) that model, track it with dvc, and use it later with unseen data in the evaluation stage.\n",
    "\n",
    "We'll import sklearn's `RandomForestRegressor()` and python's `pickle` module, load our train set, and start our training with 100 estimators and a seed. Feel free to change these parameters however you like though."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3953d600-16bf-4a07-b19c-eafd1cf3559b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d46855d-c5a0-4f2b-b35f-992581bb4128",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = pd.read_csv(os.path.join('data', '03_part', 'processed', 'train.csv'))\n",
    "y_train = X_train.pop('rented_bike_count')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02fe66a5-14d1-41d7-bef2-425c9dd59e19",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 42\n",
    "n_est = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac3cb0d2-71dc-4377-bcf9-bf7eb820195a",
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestRegressor(n_estimators=n_est, random_state=seed)\n",
    "rf.fit(X_train.values, y_train.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e2ce51c-ba62-41af-ab51-2c581ab06d5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "rf.predict(X_train.values)[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f1527f8-cc31-44c6-ad83-cc62db880d17",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('models/rf_model.pkl', \"wb\") as fd:\n",
    "    pickle.dump(rf, fd)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0842c9a3-4409-42c1-8fcb-b93d19b22f78",
   "metadata": {},
   "source": [
    "Now that we have a trained model, let's save the steps we just took to a file called `train.py`, and let's also start tracking our model in the same way in which we tracked our data earlier with dvc. Lastly, we'll commit our work and push everything to GitHub."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60b4559f-887d-46d1-bff0-1fa417947eb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile src/full_pipe/train.py\n",
    "\n",
    "import os, pickle, sys\n",
    "import numpy as np, pandas as pd\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "input_data = sys.argv[1]\n",
    "output = os.path.join('models', 'rf_model.pkl')\n",
    "seed = 42\n",
    "n_est = 100\n",
    "\n",
    "X_train = pd.read_csv(input_data)\n",
    "y_train = X_train.pop('rented_bike_count')\n",
    "\n",
    "rf = RandomForestRegressor(n_estimators=n_est, random_state=seed)\n",
    "rf.fit(X_train.values, y_train.values)\n",
    "\n",
    "with open(output, \"wb\") as fd:\n",
    "    pickle.dump(rf, fd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a2a90fe-aafe-432e-802e-74d2d1e0e936",
   "metadata": {},
   "outputs": [],
   "source": [
    "!dvc add models/rf_model.pkl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5615068-5e5b-4105-b8e0-7716bcca0324",
   "metadata": {},
   "outputs": [],
   "source": [
    "!dvc push"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ebc1150-8f20-4d73-9671-0c5dc88a4527",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "git add .\n",
    "git commit -m \"Training 1 completed\"\n",
    "git push"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83c90d0a-28c7-4038-bda2-8968cb0d119d",
   "metadata": {},
   "source": [
    "## 6. Model Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6247e0a-4c58-4f72-b0ac-e91387088c2e",
   "metadata": {},
   "source": [
    "Model evaluation is a crucial part of training ML models, and it is important that we pick useful metrics that can indicate to us how well our model is perfoming, or expecting to perform, when presented with unseen data.\n",
    "\n",
    "The metrics we'll use are Mean Absolute Error, Root Mean Squared Error, and $R^2$.\n",
    "\n",
    "- Mean Absolute Error - \"is a measure of errors between paired observations expressing the same phenomenon. Examples of Y versus X include comparisons of predicted versus observed, subsequent time versus initial time, and one technique of measurement versus an alternative technique of measurement.\" ~ [Wikipedia](https://en.wikipedia.org/wiki/Mean_absolute_error)\n",
    "- Root Mean Squared Error - \"is a frequently used measure of the differences between values (sample or population values) predicted by a model or an estimator and the values observed. The RMSD serves to aggregate the magnitudes of the errors in predictions for various data points into a single measure of predictive power. RMSD is a measure of accuracy, to compare forecasting errors of different models for a particular dataset and not between datasets, as it is scale-dependent.\" ~ [Wikipedia](https://en.wikipedia.org/wiki/Root-mean-square_deviation)\n",
    "- $R^2$ - \"In statistics, the coefficient of determination, also spelt coÃ«fficient, denoted $R^2$ or r2 and pronounced \"R squared\", is the proportion of the variation in the dependent variable that is predictable from the independent variable(s).\" ~ [Wikipedia](https://en.wikipedia.org/wiki/Coefficient_of_determination)\n",
    "\n",
    "We'll start by loading our model and our test set from the previous step, we'll then predict the # of bikes in the test set and compare such predictions with the ground truth. After we compute the metrics above, we want to save them to a JSON file for further use and comparison using dvc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fa405bd-8ed9-450c-ae91-509a3fe94b07",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.metrics as metrics, json, numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5ca84a5-b201-437c-ba66-57ad1fe1a291",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join('models', 'rf_model.pkl'), \"rb\") as fd:\n",
    "    model = pickle.load(fd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f8fc6af-f810-47cf-b445-08cdb1d0dcaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = pd.read_csv(os.path.join('data', '03_part', 'processed', 'test.csv'))\n",
    "y_test = X_test.pop('rented_bike_count')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78e95864-5e5b-4593-8b29-c7839576b50e",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.predict(X_test.values)\n",
    "predictions[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "671c9bae-07e8-4b0d-8dd1-2df5c27ba266",
   "metadata": {},
   "outputs": [],
   "source": [
    "mae = metrics.mean_absolute_error(y_test.values, predictions)\n",
    "rmse = np.sqrt(metrics.mean_squared_error(y_test.values, predictions))\n",
    "r2_score = model.score(X_test.values, y_test.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4827685-55a1-4053-9499-ae5fb73b06f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Mean Absolute Error: {mae:.2f}\")\n",
    "print(f\"Root Mean Square Error: {rmse:.2f}\")\n",
    "print(f\"R^2: {r2_score:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f87ee94-9714-4c39-bbd3-5d1c3e9e47e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join('metrics', 'metrics.json'), \"w\") as fd:\n",
    "    json.dump({\"MAE\": mae, \"RMSE\": rmse, \"R^2\":r2_score}, fd, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e491542c-9ea8-4458-b2c6-9ddc2570feb1",
   "metadata": {},
   "source": [
    "We will save the steps above in a file called `evaluate.py` for futher use later, and we will add our metrics to git and GitHub rather than to our remote gdrive bucket. The reason beign that dvc has a special function that allows us to compare the diff of metrics between those in a branch and those in master or those between different commits, and as you'll see soon, this is a very powerful feature of dvc that we certainly want to take advantage of."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f29884bb-80df-4cfc-b399-03c8eaae3d8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile src/full_pipe/evaluate.py\n",
    "\n",
    "import json, os, pickle, sys, pandas as pd, numpy as np\n",
    "import sklearn.metrics as metrics\n",
    "\n",
    "model_file = sys.argv[1]\n",
    "test_file = os.path.join(sys.argv[2], \"test.csv\")\n",
    "scores_file = os.path.join('metrics', 'metrics.json')\n",
    "\n",
    "with open(model_file, \"rb\") as fd:\n",
    "    model = pickle.load(fd)\n",
    "\n",
    "X_test = pd.read_csv(test_file)\n",
    "y_test = X_test.pop('rented_bike_count')\n",
    "\n",
    "predictions = model.predict(X_test.values)\n",
    "\n",
    "mae = metrics.mean_absolute_error(y_test.values, predictions)\n",
    "rmse = np.sqrt(metrics.mean_squared_error(y_test.values, predictions))\n",
    "r2_score = model.score(X_test.values, y_test.values)\n",
    "\n",
    "with open(scores_file, \"w\") as fd:\n",
    "    json.dump({\"MAE\": mae, \"RMSE\": rmse, \"R^2\":r2_score}, fd, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "277f2431-abeb-4e56-9101-391b254080ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "git add .\n",
    "git commit -m \"Evaluation completed\"\n",
    "git push"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b42a8c6-2815-4950-bca5-885a2461864f",
   "metadata": {},
   "source": [
    "## 7. DVC Pipelines"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4b1c6f0-aee8-47d3-9399-b4ffdf42a8ea",
   "metadata": {},
   "source": [
    "DVC pipelines is one of the best features offered by dvc. They allow us to create reproducible workflows containing anything from getting the data to training and evaluating models.\n",
    "\n",
    "There are several ways for creating pipelines with dvc and here we'll do so with `dvc run`. `dvc run` starts with the `-n` flag followed by the name we want to give to the step of the pipeline we want to create. Next, we add the `-d` flag to signal dependencies such as the python file we want to run as well as any arguments that such file takes. Next we have the `-o` flag, which tells dvc the output expected from such step of the pipeline. For example, this stage would take the `train.csv` and `test.csv` files from the data preparation stage. Lastly, you need to pass the full python call without any flags.\n",
    "\n",
    "After we run our dvc command we'll get 2 files, a `dvc.yaml` and a `dvc.lock` file. The former contains the stages dvc will follow in our pipeline, and the latter contains the metadata and other information regarding our pipeline. Once you have a look at the yaml file, you'll probably wonder if you can create such a file manually, the answer is yes. For the `dvc.lock` on the other hand, dvc will take care of that one through the command `dvc repro`, which runs whatever pipeline resides in your `dvc.yaml` file.\n",
    "\n",
    "More information about both can be found in the official documentation site [here](https://dvc.org/doc/command-reference/run).\n",
    "\n",
    "Before we start adding the stages of our pipeline, let's first remove the files we were already tracking with dvc. Not doing so will result in dvc giving us an error since the tracked files already exist, and also, we want to see how the whole pipeline behaves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7b55e69-6017-4f6f-a934-56790278dad0",
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm dvc.lock dvc.yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70426e36-e515-453f-988a-88abfb4119a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "dvc remove data/03_part/raw/SeoulBikeData.csv.dvc \\\n",
    "           data/03_part/processed/train.csv.dvc \\\n",
    "           data/03_part/processed/test.csv.dvc \\\n",
    "           models/rf_model.pkl.dvc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "940e8e5d-ba3e-49ab-83ef-08308d982dcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "dvc run -n get_data \\\n",
    "    -d src/full_pipe/get_data.py \\\n",
    "    -o data/03_part/raw/SeoulBikeData.csv \\\n",
    "    python src/full_pipe/get_data.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "304081fb-b4e7-4f10-8459-c8bba4ab7cd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "dvc run -n prepare \\\n",
    "    -d src/full_pipe/prepare.py -d data/03_part/raw/SeoulBikeData.csv \\\n",
    "    -o data/03_part/processed/train.csv -o data/03_part/processed/test.csv \\\n",
    "    python src/full_pipe/prepare.py data/03_part/raw/SeoulBikeData.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccee8a35-52f6-449f-a3e4-9b073ee62bf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "dvc run -n train \\\n",
    "    -d src/full_pipe/train.py -d data/03_part/processed/train.csv \\\n",
    "    -o models/rf_model.pkl \\\n",
    "    python src/full_pipe/train.py data/03_part/processed/train.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e5d96a3-aa44-45f5-91db-a8a45ff0cceb",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "dvc run -n evaluate \\\n",
    "    -d src/full_pipe/evaluate.py -d models/rf_model.pkl -d data/03_part/processed \\\n",
    "    -M metrics/metrics.json \\\n",
    "    python src/full_pipe/evaluate.py models/rf_model.pkl data/03_part/processed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0972b3ad-2ff6-417c-abd9-88b30f8b4987",
   "metadata": {},
   "source": [
    "Notice that in the last part of our pipeline we have the flag `-M`. This flag tells dvc to treat the output of that particular stage as a metric so that we can later use `dvc diff` on it and compare metrics between diffs or between branches in git.\n",
    "\n",
    "Using the `dvc status` will tells us whether there are changes in our pipeline and files or if everthing is up to date."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11b45e5f-dda1-4dec-8443-bd4b7f313525",
   "metadata": {},
   "outputs": [],
   "source": [
    "!dvc status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37e783fc-2798-4535-a651-a7a98d26a07f",
   "metadata": {},
   "outputs": [],
   "source": [
    "!dvc metrics show"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16d093e1-076c-4769-8910-3ccbc8c6ff52",
   "metadata": {},
   "source": [
    "Another cool function of dvc is `dvc dag`, which will show us a graph with the steps in our pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0ffb172-1e1d-4928-a956-387944a28ec5",
   "metadata": {},
   "outputs": [],
   "source": [
    "!dvc dag"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18d8a6b0-46ab-40c6-95cb-d42d9829f8dd",
   "metadata": {},
   "source": [
    "In order to re-run our pipeline again with `dvc repro` and see it in action, let's lemove the `dvc.lock` and the data files, and run `dvc repro` once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53158be4-a3d7-44eb-8051-326ea10df068",
   "metadata": {},
   "outputs": [],
   "source": [
    "!dvc repro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2155d6d9-8126-4d8b-8a0f-e06e66189628",
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm dvc.lock data/03_part/raw/SeoulBikeData.csv data/03_part/processed/train.csv data/03_part/processed/test.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80132da7-0a83-42b2-a86c-2dd37b0fa401",
   "metadata": {},
   "outputs": [],
   "source": [
    "!dvc repro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0410d991-5c27-43da-9667-37464b52b7ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "!dvc push"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea935677-d0d7-4d6a-a0b6-da770bbbdcc3",
   "metadata": {},
   "source": [
    "Now that we have learned about dvc pipelines and how to reproduce them, let's check the files that need to be committed and let's push them to GitHub."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94f4afc9-dd33-47b3-a498-8b07fc7a625e",
   "metadata": {},
   "outputs": [],
   "source": [
    "!git status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e64290a-66de-42e7-b2cf-8d83344ccaac",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "git add .\n",
    "git commit -m \"Pipeline Finished\"\n",
    "git push"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74405837-7685-42eb-a2ca-3e0ada66b0f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "!git status"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5cf1b6b-9bbb-4812-a594-4d7ed89a685a",
   "metadata": {},
   "source": [
    "## 8. Experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4618ae71-706e-4e04-9f37-f11ba9261751",
   "metadata": {},
   "source": [
    "We've been working inside our master branch using scikit-learn, and now we want to start experimenting with other tree-based frameworks like XGBoost, LightGBM, and CatBoost using different commits and/or branches for each experiment. Let's do just that and start by checking out a new branch, adding XGBoost to our train file, and triggering a new run.\n",
    "\n",
    "**Note:** although the explanation is in terms of git branches we'll try to stay in the master branch for the workshop purposes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bd70cb5-9d1f-44fa-8473-0bfbdbc38f3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !git checkout -b \"exp1-xgb\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "152cfddc-d6c9-47af-b0b6-311c21e49164",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile src/full_pipe/train.py\n",
    "\n",
    "import os, pickle, sys, pandas as pd\n",
    "from xgboost import XGBRFRegressor\n",
    "\n",
    "input_data = sys.argv[1]\n",
    "output = os.path.join('models', 'rf_model.pkl')\n",
    "seed, n_est = 42, 100\n",
    "\n",
    "X_train = pd.read_csv(input_data)\n",
    "y_train = X_train.pop('rented_bike_count')\n",
    "\n",
    "rf = XGBRFRegressor(n_estimators=n_est, seed=seed)\n",
    "rf.fit(X_train.values, y_train.values)\n",
    "\n",
    "with open(output, \"wb\") as fd: pickle.dump(rf, fd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2a9d7a4-e755-48a1-bb22-aaa322b7af7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "!dvc status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "302f4bc2-a664-44a5-9d08-f96bd60ec9ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "!dvc repro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18b001be-c19f-4bbd-a6ce-d90179c3f45b",
   "metadata": {},
   "outputs": [],
   "source": [
    "!dvc status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f1353d4-41b4-452b-98fb-60232af182cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "!dvc push"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee5128f6-60f7-475b-9883-f94ae6f52716",
   "metadata": {},
   "outputs": [],
   "source": [
    "!dvc metrics show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5fd561a-a6c9-449e-8f54-f0afb899d676",
   "metadata": {},
   "outputs": [],
   "source": [
    "!dvc metrics diff --show-md master"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e90c2fa0-7ccf-4253-8fba-c9ce63b3ea2f",
   "metadata": {},
   "source": [
    "Note that in order for GitHub to now we have been working in a different branch, we need to use the `git push --set-upstream origin exp1-xgb` command. Otherwise, we'll get an error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8170f9d-7471-4867-8537-fce3a8ba26b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "git add .\n",
    "git commit -m \"Testing XGBoost\"\n",
    "git push\n",
    "# git push --set-upstream origin exp1-xgb\n",
    "# git push"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbc447c7-1ee3-4b79-b1e1-c1a9a9a990fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !git checkout -b \"exp2-lgbm\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcaa78b4-aa5e-4a32-90ce-ba5db417475e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile src/full_pipe/train.py\n",
    "\n",
    "import os, pickle, sys, pandas as pd\n",
    "from lightgbm import LGBMRegressor\n",
    "\n",
    "input_data = sys.argv[1]\n",
    "output = os.path.join('models', 'rf_model.pkl')\n",
    "seed, n_est = 42, 100\n",
    "\n",
    "X_train = pd.read_csv(input_data)\n",
    "y_train = X_train.pop('rented_bike_count')\n",
    "\n",
    "rf = LGBMRegressor(n_estimators=n_est, random_state=seed)\n",
    "rf.fit(X_train.values, y_train.values)\n",
    "\n",
    "with open(output, \"wb\") as fd: pickle.dump(rf, fd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ade36dc-9574-4282-9e31-ff3d13d9a70b",
   "metadata": {},
   "outputs": [],
   "source": [
    "!dvc status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5c8bd0f-9638-4e96-aac0-16b97584ba9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "!dvc repro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23d5333d-7d46-4265-b684-c1790fa4ad57",
   "metadata": {},
   "outputs": [],
   "source": [
    "!dvc push"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32e36d61-b83e-44d7-9ff3-db894e17091e",
   "metadata": {},
   "outputs": [],
   "source": [
    "!dvc metrics diff --show-md master"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1031445f-3935-461b-b5b1-2c62599f2fc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "git add .\n",
    "git commit -m \"Testing LightGBM\"\n",
    "git push\n",
    "# git push --set-upstream origin exp2-lgbm\n",
    "# git push"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "251c8b37-6d23-4c92-a5e8-322a4cbf1506",
   "metadata": {},
   "source": [
    "The base implementation of LightGBM seems to have performed quite well. Let's try out CatBoost now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96db5255-9607-42cb-87a0-c21e02428de5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !git checkout -b \"exp3-cat\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cee8833d-2086-4769-8ee5-7ae4538e28c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile src/full_pipe/train.py\n",
    "\n",
    "import os, pickle, sys, pandas as pd\n",
    "from catboost import CatBoostRegressor\n",
    "\n",
    "input_data = sys.argv[1]\n",
    "output = os.path.join('models', 'rf_model.pkl')\n",
    "seed, n_est = 42, 100\n",
    "\n",
    "X_train = pd.read_csv(input_data)\n",
    "y_train = X_train.pop('rented_bike_count')\n",
    "\n",
    "rf = CatBoostRegressor(n_estimators=n_est, random_state=seed)\n",
    "rf.fit(X_train.values, y_train.values)\n",
    "\n",
    "with open(output, \"wb\") as fd: pickle.dump(rf, fd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6f0fe0b-acca-4de0-a9b4-926d8f8dd9fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "!dvc status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a3865ea-c80b-4033-b50d-1f176cfaeee5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!dvc repro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a108c6f-9620-41bd-9c3b-ea8ecb897465",
   "metadata": {},
   "outputs": [],
   "source": [
    "!dvc push"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "992ab857-dbbe-4c96-a043-0d65e11740c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "!dvc metrics diff --show-md master"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ab99a16-b2e7-4045-93fd-39a0cf5e3cab",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "git add .\n",
    "git commit -m \"Testing CatBoost\"\n",
    "git push\n",
    "# git push --set-upstream origin exp3-cat\n",
    "# git push"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "215aaaf3-e6d1-450a-ac79-8c82a3c9f42e",
   "metadata": {},
   "source": [
    "Surprisingly, CatBoost's MAE performed a bit worse than LightGBM but RMSE performed much better.\n",
    "\n",
    "We have a good candidate with CatBoost and we should merge this branch with master and start tunning our model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6e4b759-bac7-4503-94c9-9eaaaf5c5648",
   "metadata": {},
   "source": [
    "## 9. Merging our Changes - PRs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bff1fa0-20ca-4e7b-81a4-94f79c79c7f4",
   "metadata": {},
   "source": [
    "![empspr](../images/compare_pr.png)\n",
    "If you go back to the main page of our repo, you'll notice that GitHub has added a **Compare & pull request** option for each the three experiments. This is a nice shortcut to help us pick the one we liked best and add it to our main project's branch, master.\n",
    "\n",
    "So what is a pull request anyways? \"A PR provides a user-friendly web interface for discussing proposed changes before integrating them into the official project.\" ~ [Atlassian](https://www.atlassian.com/git/tutorials/making-a-pull-request)\n",
    "\n",
    "Let's merge our experiment branch with our master branch.\n",
    "\n",
    "1. Click on the **Compare & pull request** for exp3-cat.\n",
    "2. Compare the changes.\n",
    "![comp](../images/compare_pr.png)\n",
    "3. Check out the report again.\n",
    "![rep](../images/report_bottom.png)\n",
    "4. Open a pull request with your details on why it should go to master.\n",
    "![rep](../images/pr_mess.png)\n",
    "5. Once reviewed, write a comment and merge the pull request.\n",
    "![rep](../images/awesome_cat.png)\n",
    "6. Lastly, we need to make sure our local env is up to date and once it is, we can switch to the master branch and work from there again. Run a `git pull` and a `dvc pull`.\n",
    "![list_prs](../images/last_of_pr.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1aab025a-ed5e-4190-959c-77734b68e4cb",
   "metadata": {},
   "source": [
    "## 10. Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9e638f6-61ea-4371-bbca-203450647d7a",
   "metadata": {},
   "source": [
    "As you have seen throughout the tutorial\n",
    "\n",
    "DVC helps us track our data, models, and metrics, and it also allows us to create pipelines for getting, preparing, and modeling data.\n",
    "\n",
    "DVC fills in the gaps of what git alone can't do for the machine learning community. This tool should be in every data scientist and ML Engineer's toolkit. Enough said!\n",
    "\n",
    "![enough](https://media.giphy.com/media/mVJ5xyiYkC3Vm/giphy.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0ce9603-232b-49da-9df5-96a113fae284",
   "metadata": {},
   "source": [
    "## 11. Blind Spots and Future Work"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "057a8d49-405f-441f-910d-4f9ff78cc391",
   "metadata": {},
   "source": [
    "**Blind Spots**\n",
    "- We could have fine tuned our base model even further and make better comparisons with the other frameworks.\n",
    "- We could have conducted more feature engineering.\n",
    "- We could have selected the best features only based on feature importance.\n",
    "- We could have done a bit more analysis of the data.\n",
    "- We could have taken out the second dummy or our categorical variables. For example, there is no need to have Holiday and No Holiday as variables in our dataset.\n",
    "\n",
    "\n",
    "**Future Work**\n",
    "- If the data will be provided in the same formate we received it, then we need an easier transformation pipeline for the date, column names, and dummies.\n",
    "- We could add the analytical tool, e.g. our dashboard, to the master branch and work with the models solely through branches."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1a08954-2792-4f3e-a787-100500ecd109",
   "metadata": {},
   "source": [
    "## 12. Resources"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "720f2d19-a43e-40b0-af52-0ced35baee4b",
   "metadata": {},
   "source": [
    "Here are a few additional resources to dive deeped into some of the tools discussed above.\n",
    "- [DVC Get Started](https://dvc.org/doc/start)\n",
    "- [DVC Use Cases](https://dvc.org/doc/use-cases)\n",
    "- [CML Get Started](https://cml.dev/doc/start)\n",
    "- [CatBoost Tutorial](https://catboost.ai/en/docs/concepts/tutorials)\n",
    "- [Git](https://realpython.com/python-git-github-intro)\n",
    "- [Pull Requests](https://www.atlassian.com/git/tutorials/making-a-pull-request)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
